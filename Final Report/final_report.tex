\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Final Report},
            pdfauthor={Li Liu, Abhishek Pandit, Adam Shelton},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\usepackage{setspace}\doublespacing
% https://github.com/rstudio/rmarkdown/issues/337
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

% https://github.com/rstudio/rmarkdown/pull/252
\usepackage{titling}
\setlength{\droptitle}{-2em}

\pretitle{\vspace{\droptitle}\centering\huge}
\posttitle{\par}

\preauthor{\centering\large\emph}
\postauthor{\par}

\predate{\centering\large\emph}
\postdate{\par}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\title{Final Report}
\author{Li Liu, Abhishek Pandit, Adam Shelton}
\date{12/7/2019}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{contributions}{%
\section{Contributions}\label{contributions}}

\begin{table}[H]
\centering
\begin{tabular}{l|l|l}
\hline
\textbf{Liu} & \textbf{Pandit} & \textbf{Shelton}\\
\hline
KMeans & Item 1 & EDA\\
\hline
Structural Topic Modeling & Item 2 & AGNES\\
\hline
Discussion & Item 3 & DBSCAN\\
\hline
\end{tabular}
\end{table}

\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

âSo tell me about yourself!ââ This seemingly straightforward
question in day-to-day interactions is usually met with silence and
hesitation. That can no longer be the case for the 1.67 trillion online
dating industry, which has grown exponentially in popularity over the
last decade. The dating apps, such as OkCupid and Coffee Meets Bagel,
are designed to help the singles âget to knowâ other people for
short or long-term romantic relationships. In order to be popular and
memorable, users usually have to write a short introduction to advertise
themselves. Such activity could be regarded as self-marketing. As the
users of dating apps come from diverse backgrounds, we are interested in
how users from distinct backgrounds take different approaches to make
themselves more memorable. Moreover, we design the framework of scoring
usersâ self-introduction and algorithm for providing writing tips
(such as words for being memorable). Although our project is still
preliminary, it has gained a lot of interest among our friends who
struggle to find a date online. Also, our methods and analysis have the
potential to be adopted by the dating website to improve the usersâ
experience and better achieve their mission as matchmakers.

\hypertarget{literature-review}{%
\section{Literature Review}\label{literature-review}}

Self-concept and self-representation have long served as grounds of
debate in cognitive and positive psychology (Bruning, Schraw, and
Ronning 1999) as well as social anthropology (Goffman 1975). The recent
spread of social networking and its specific affordances have allowed
individuals to build different online âselvesâ (Papacharissi 2010).
One such critical scenario may be that of mate selection, which several
economists and sociologists have likened this to âmarriage
marketplaceâ (Hitsch, Hortaçsu, and Ariely 2010). Several online
dating service providers in developed countries may facilitate the
expansion of potential mates beyond the limits of even extended offline
social networks Cacioppo et al. (2013) assert that as many as one in
three marriages in the United States is facilitated through these
portals. Heino, Ellison, and Gibbs (2010) argue that these avenues
further entrench the economic dimension through an acute, implicit
awareness of ârelationshoppingâ. Herein, potential partners are
reduced to entries in a catalog to be scrolled through. In this sense,
they suggest an emerging conscientiousness of âmarketingâ, with the
product being themselves, and the potential mate assuming the role of a
buyer (ibid). This perception thus links the private worlds of romantic
intimacy with those of mass consumption and broader perceived appeal to
the opposite sex.

Potentially, we will also use some marketing theories to understand our
findings. Selling themselves and finding a mate on OkCupid is not very
different from selling a product on eBay. Economists have been
interested in the matching problem of demand and supply, such as Hitsch,
Hortaçsu, and Ariely (2010). Since we do not have data on usersâ
interactions, we will focus primarily on understanding how people brand
themselves to stand out in a crowd. For example, brand awareness is a
key metric in marketing to quantify the degree to which people recall or
recognize a brand. A high level of brand awareness helps a product stand
out and get chosen when consumers face many alternatives.

This could be applied to understand online dating. Let us imagine your
future mate uses the filter to narrow down the consideration sets.
He/She might still face many similar choices with high matching scores
to choose from. If you want to stand out from the pool, you must make
yourself memorable by highlighting the uniqueness. Thus, one possible
idea in this project is to explore and understand how users could
increase their brand awareness and differentiate themselves in their
segments

\hypertarget{empirical-strategy}{%
\section{Empirical Strategy}\label{empirical-strategy}}

\hypertarget{analysis-results}{%
\section{Analysis \& Results}\label{analysis-results}}

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory Data Analysis}\label{exploratory-data-analysis}}

\hypertarget{descriptive-statistics}{%
\subsubsection{Descriptive Statistics}\label{descriptive-statistics}}

\begin{table}

\caption{\label{tab:descr-stats-demo}Continuous Variables}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100\\
\hline
age & 0 & 1 & 32.02 & 9.09 & 18.0 & 26.00 & 30.00 & 36.00 & 69\\
\hline
height & 0 & 1 & 70.51 & 3.03 & 3.0 & 69.00 & 70.00 & 72.00 & 95\\
\hline
long\_words & 0 & 1 & 11.33 & 13.28 & 0.0 & 3.00 & 8.00 & 15.00 & 446\\
\hline
flesch & 0 & 1 & 7.30 & 4.75 & -3.6 & 4.86 & 6.73 & 8.96 & 268\\
\hline
profile\_length & 0 & 1 & 117.84 & 122.21 & 1.0 & 43.00 & 85.00 & 153.00 & 2973\\
\hline
prop\_longwords & 0 & 1 & 0.10 & 0.08 & 0.0 & 0.06 & 0.09 & 0.12 & 3\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:descr-stats-demo}Other Variables}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & min & max & empty & n\_unique & whitespace\\
\hline
clean\_text & 0 & 1 & 1 & 16952 & 0 & 18790 & 0\\
\hline
essay9 & 0 & 1 & 1 & 10849 & 0 & 18125 & 0\\
\hline
edu & 0 & 1 & 7 & 21 & 0 & 3 & 0\\
\hline
fit & 0 & 1 & 3 & 7 & 0 & 3 & 0\\
\hline
race\_ethnicity & 0 & 1 & 5 & 8 & 0 & 6 & 0\\
\hline
height\_group & 0 & 1 & 5 & 9 & 0 & 2 & 0\\
\hline
\end{tabular}
\end{table}

\includegraphics{final_report_files/figure-latex/descr-stats-demo-1.pdf}

The data we used was approximately 60,000 anonymous OkCupid profiles
from 2012 that were gathered with consent from users in the San
Francisco area (Kim and Escobedo-Land 2015). This data was downloaded
from the GitHub page for Kim and Escobedo-Land (2015),
\url{https://github.com/rudeboybert/JSE_OkCupid}. The data contains
demographic attributes of users that were submitted to their profile,
including variables like age, height, race, and education, in addition
to a selection of ten short essays that users have written in response
to different prompts to display on their profiles. We subsetted this
data to 18817 profiles of men, and generated additional features for the
numbers and proportions of long words and FleschâKincaid readability
scores of the main profile essay. The majority of male users in our
sample or white, fit, not-short, and have more than a high school
education. The mean reported age is 32 and the mean reported height is
70.5 inches (approximately 5 foot 9 inches).

The majority of the variables included in the demographic data are
independent, but some weaker correlations do exist. As expected there
are positive correlations between all the features generated from the
essay text. Age is also positively correlated with our text-generated
features, perhaps suggesting that older people are more educated and
write with more complexity. While Flesch scores and the amount of long
words are correlated, there do not appear to be any demographic
interactions with that relationship.

\includegraphics{final_report_files/figure-latex/viz-analysis-1.pdf}
\includegraphics{final_report_files/figure-latex/viz-analysis-2.pdf}

\hypertarget{clusterability}{%
\subsubsection{Clusterability}\label{clusterability}}

\includegraphics{final_report_files/figure-latex/demo-clusterability-1.pdf}
The demographic data is highly clusterable with a Hopkins Statistic of
0.927 on a random subset of 2,000 observations. Unfortuantely,
clusterability could not be determined for the whole dataset due to
performance limitations. However, 2,000 observations should be
sufficient for determining clusterability.

\hypertarget{clustering-of-demographic-data}{%
\subsection{Clustering of Demographic
Data}\label{clustering-of-demographic-data}}

\hypertarget{k-means}{%
\subsubsection{K-means}\label{k-means}}

\hypertarget{agnes}{%
\subsubsection{AGNES}\label{agnes}}

Agglomerative Nesting was used to cluster the demographic data with a
bottom-up approach to contrast the K-means clustering. As an AGNES model
would not complete on the full data-set, a random subset of 5,000
observations was used instead. This AGNES model used a Gower's
dissimilarity matrix of the data to aid in the clustering of categorical
variables, since most of the demographic variables are categorical.
Using the \texttt{NbClust} package in R, we determined that the optimal
number of clusters was two, when using the \texttt{ward.D2} method to
match our AGNES model. The model gives us two well-defined clusters
along the first two principal components. The defining factor between
these two clusters, majorly appears to be height, which, per the Wilcox
Test, has a statistically significant difference between the means of
the two clusters by about five inches.

\begin{figure}
\centering
\includegraphics{../Clustering/advanced_clustering_files/figure-gfm/agg-nest-3.png}
\caption{NbClust Results}
\end{figure}

\begin{figure}
\centering
\includegraphics{../Clustering/advanced_clustering_files/figure-gfm/agg-nest-4.png}
\caption{AGNES Dendrogram}
\end{figure}

\begin{figure}
\centering
\includegraphics{../Clustering/advanced_clustering_files/figure-gfm/agg-nest-5.png}
\caption{AGNES Cluster Results}
\end{figure}

\hypertarget{text-analysis}{%
\subsection{Text Analysis}\label{text-analysis}}

\hypertarget{word2vec}{%
\subsubsection{Word2Vec}\label{word2vec}}

\hypertarget{topic-modeling}{%
\subsubsection{Topic Modeling}\label{topic-modeling}}

A topic is defined as a mixture of words where each word has a
probability of generating from a topic. The example would be words such
as `books', `college', `MOOC' could come from the topic of study. A
document is a mixture of topics, where a single document can have
covered multiple topics. The example would be that a user talks about
topics related to career, study, hobbies, and religion in the
self-introduction.

\hypertarget{dbscan}{%
\subsubsection{DBSCAN}\label{dbscan}}

As we wanted to determine factors behind why a profile might stick out,
we decided to use a DBSCAN model was used to detect outliers within a
data-set of 50 vectors calculated by Doc2Vec. As Doc2Vec vectors capture
different characteristics about the text, any outliers in these vectors
should be profiles that deviate from the norm to some degree. As shown
below, these Doc2Vec vectors are highly independent with very few
correlations, but also highly clusterable, with a Hopkins Statistic of
0.808.

\begin{figure}
\centering
\includegraphics{../Clustering/dbscan_doc2vec_files/figure-gfm/data-1.png}
\caption{Doc2Vec Correlation Plot}
\end{figure}

Using a K-nearest-neighbors distribution plot, we determined that the
optimal value for the epsilon neighborhood size of the DBSCAN model was
9. This was determined using 5 nearest neighbors, to match the default
minimum number of points in the epsilon region, which we used for the
model.

\begin{figure}
\centering
\includegraphics{../Clustering/dbscan_doc2vec_files/figure-gfm/dbscan-1.png}
\caption{DBSCAN KNN Distribution Plot}
\end{figure}

The DBSCAN model identified one cluster, and 108 outliers, accounting
for 0.57\% of the observations. Using a Wilcox Test, we can determine
that the difference in the means between the ``typical'' profiles and
outlier profiles are insignificant for the continuous variables for
height and number of long words, but are highly significant for Flesch
score and age. Mean age is higher among the outliers by about 4.5 years,
while the mean Flesch score of outliers is about 8 points higher. This
suggests that the outliers, being older and writing at a more advanced
level, might be more educated (or at least trying to appear so) than
their ``typical'' peers.

\begin{figure}
\centering
\includegraphics{../Clustering/dbscan_doc2vec_files/figure-gfm/dbscan-2.png}
\caption{DBSCAN Cluster Plot}
\end{figure}

\hypertarget{combining-text-and-demographic-data}{%
\subsection{Combining Text and Demographic
Data}\label{combining-text-and-demographic-data}}

\hypertarget{motivation}{%
\subsubsection{Motivation}\label{motivation}}

So far, we have explored the patterns within the demographic variables
using clustering and self-introduction text using LDA. However, in order
to quantify whether people of different backgrounds write different
topics, we need to model the topic distributions as a function of the
metadata (demographic variables and DBScan score). The structural topic
modeling (STM) solves the problem as it allows researchers to discover
topics and estimate their relationships to document metadata (Roberts et
al.~2016). Similar to the Latent Dirichlet Allocation model, STM also
assumes there are some latent document-topic and topic-word
distributions generating documents. However, STM differs from LDA as it
handles the document-associated metadata. As a result, STM allows us to
predict how topical prevalence (proportion of a topic across multiple
documents) or topic content (the topic composition of terms) would shift
when the metadata changes.

\hypertarget{estimation}{%
\subsubsection{Estimation}\label{estimation}}

We use the \texttt{stm} R package to estimate the model, summarize
results, and visualize with the word cloud and topic network. In our
project, we are interested in the topical prevalence. So the response (
dependent variable) is the proportion of a topic across multiple
self-introductions. The metadata (independent variables) is `fit',
`education', `height\_group', `race\_ethnicity', and `dbscan\_cluster'.

In the preparation step, we use \texttt{textProcessor} function to stem
the words and remove stopwords. Then we use \texttt{prepDocuments}
function to structure and index the data. As the low-frequency words are
probably the more memorable ones, we set the `lower.thresh' option to 0
to keep all words.

Then we estimate the model for the topic prevalence using 9 topic
models. During the estimation, the proportions of 9 topics are regressed
on the metadata. There are many ways to visualize the results. The
following plot shows the expected topic proportions

\begin{figure}
\centering
\includegraphics{Plots/toptopic.jpeg}
\caption{Top Topic}
\end{figure}

The proportions across 9 topics do not have a large gap, as the most
common topic is 0.14 and the least common one is 0.10. We observe that
the top three topics seem to consist of common nouns and verbs that are
not very memorable. So we further explore the most frequent words in the
model for both the most common topics (\#5, \#4, and \#7) and the least
common topics (\#1, \#2, and \#9).

\includegraphics{Plots/top3topic.jpeg}
\includegraphics{Plots/bottom3topic.jpeg}

Although it's hard to tell the quantified difference, we find the words
in the least common topics are a little more memorable, such as
`intelligent' and `dance'.

When we plot the histograms of topics, we find that most of the topics
consist of around 10\% of the topics of the documents. Interestingly,
the histograms of topics \#6 and \#8 seem to have a bell-curve shape,
which suggests there are many documents have higher or lower proportions
of 10\%. \includegraphics{Plots/hist.jpeg}

In particular, we plot these two plots on the same line. Topic \#6 is
about `friend' and `live' and topic \#8 is about `year' and `get'. So
our interpretation is that topic \#6 s about casually looking for
friends while topic \#8 is about setting a goal of getting something new
in the year. As a result, although most users talk about them
explicitly, some talk them more and some use other expressions.
\includegraphics{Plots/comparison.jpeg}

Also, \texttt{topicCorr()} calculates correlations between topics, where
positive correlations mean that both topics are likely to be discussed
within a document.

\begin{figure}
\centering
\includegraphics{Plots/network.jpeg}
\caption{Topic Networks}
\end{figure}

For the OkCupid beginners, it will be helpful if they could learn what
topics are they expected to write together. However, this would only
make an ordinary self-introduction. To make it really memorable and
stand out, they should consider writing a few topics that are unique and
quirky.

\hypertarget{evaluation}{%
\subsubsection{Evaluation}\label{evaluation}}

To further understand whether the topics make sense, we use the
topicQuality() function to plot the semantic coherence and exclusivity
values associated with each topic.

\begin{figure}
\centering
\includegraphics{Plots/topicQuality.jpeg}
\caption{Topic Quality}
\end{figure}

The results seem to highly correlated with the topic proportions, as
topics with higher proportions also have higher semantic coherence and
exclusivity values.

\hypertarget{effect}{%
\subsubsection{Effect}\label{effect}}

Finally, we use \texttt{estimateEffect()} function to estimating
relationships between metadata and topic prevalence. There are 9
regressions under the hood and 12 coefficients associated with each. To
effectively analyze the results with 108 coefficients, we compile them
into a table by indicating the coefficients which are statistically
significant at the level of 5\%. These significant coefficients suggest
that we can reject the null hypothesis and accept the alternative
hypothesis that a relationship exists between the topic prevalence and
metadata.

\begin{figure}
\centering
\includegraphics{Plots/Covariates.png}
\caption{Significant Coefficients}
\end{figure}

We observe from the table that the factor ``edu\_More than HIgh Schoo''
is significant at all of the 9 regressions. This means that being more
educated is associated with some variations in the topic proportions.
Intuitively, this suggests more educated people might want to write
certain topics that make them memorized as intelligent and knowledgable.

We notice `height\_groupshort' is only significant for regression \#7.
Topic \# 7 has high probability words such as " like, also, think,
someone", which seems quite ordinary. It's likely the use of `also'
indicates they are disconfident when introducing themselves, which might
because they feel they are too short.

When looking at the coefficients at each regression, we are surprised
that in the regression \#6, only edu\_More than HIgh Schoo" matters for
Topic \#6's proportions. In comparison, 8 out of the 11 coefficients are
significant in the regression \#8. Our current understanding is that for
topic\#8, people with different backgrounds have various ideas of how
much to write about it.

In summary, we get some interesting results during our first time using
structural topic modeling. Although we are still learning it, we believe
it has great potential to undercover the underlying relationships
between metadata and topic prevalence. Such relationships would be
useful for OkCupid to provide users with writing tips to make their
self-introduction more memorable.

\hypertarget{output}{%
\subsection{Output}\label{output}}

The above three pictures show the protocol of the final product that we
have in mind. The protocol uses the example of Li (one of the project
members), although the information is hypothetical. The first picture
displays his demographic information and a short self-introduction. The
second picture shows that the new algorithm highlights the common and
memorable words by comparing them with the frequencies among his peers.
He also gets a score of 63 and some tips on improving the writing. The
third picture is the revised version of previous writing, which now Li
has a memorable self-introduction with an increased score of 87. He
should be confident to reach out to other users on OkCupid now.

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

The project faces several limitations at this stage. Firstly, without
data on the outcome (such as the number of messages), we do not know
have a relative measure for the self-introductions' memorabilities. As a
result, we don't have the outcomes to train the scoring function with
machine learning. Secondly, without follow-up interviews, we cannot
measure whether the specific choice of words was aimed at authenticity
or matches with an awareness of `relationshopping' (experienced users
use certain phrases fraudulently to hook others). Thirdly, without the
users' other profile images, we cannot estimate the effect of a
memorable self-introduction on outcomes as the quality of images is a
potential moderating variable.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\newpage

\hypertarget{appendices}{%
\section{Appendices}\label{appendices}}

\hypertarget{agnes-code}{%
\subsection{AGNES Code}\label{agnes-code}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## ----setup,}
\CommentTok{## include=FALSE-----------------------------------------------------}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(cluster)}
\KeywordTok{library}\NormalTok{(FactoMineR)}
\KeywordTok{library}\NormalTok{(factoextra)}
\KeywordTok{library}\NormalTok{(NbClust)}
\KeywordTok{library}\NormalTok{(dbscan)}
\KeywordTok{library}\NormalTok{(cowplot)}
\KeywordTok{library}\NormalTok{(skimr)}
\KeywordTok{library}\NormalTok{(ggcorrplot)}
\KeywordTok{library}\NormalTok{(missMDA)}
\KeywordTok{library}\NormalTok{(cluster)}
\KeywordTok{library}\NormalTok{(missForest)}
\KeywordTok{library}\NormalTok{(tictoc)}
\KeywordTok{library}\NormalTok{(doParallel)}

\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{fig.height =} \DecValTok{6}\NormalTok{, }
    \DataTypeTok{fig.width =} \DecValTok{8}\NormalTok{, }\DataTypeTok{dpi =} \DecValTok{400}\NormalTok{)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{60615}\NormalTok{)}

\CommentTok{# drops variables in a dataset with a certain}
\CommentTok{# percentage or number of missing values (a lower}
\CommentTok{# value means less missing values are allowed}
\CommentTok{# before a variable is dropped)}
\NormalTok{drop_high_na =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data_set, }\DataTypeTok{cutoff =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{rows =} \OtherTok{FALSE}\NormalTok{) \{}
    \KeywordTok{library}\NormalTok{(tidyverse)}
    \ControlFlowTok{if}\NormalTok{ (cutoff }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
        \KeywordTok{stop}\NormalTok{(}\StringTok{"Cutoff cannot be less than zero!"}\NormalTok{)}
\NormalTok{    \}}
    \CommentTok{# returns a logical value if a row/column at a}
    \CommentTok{# specifed index should be dropped}
\NormalTok{    drop_index =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(index, ds) \{}
\NormalTok{        items =}\StringTok{ }\OtherTok{NULL}
        \ControlFlowTok{if}\NormalTok{ (rows) \{}
\NormalTok{            items =}\StringTok{ }\NormalTok{ds[index, ] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{()}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{            items =}\StringTok{ }\NormalTok{ds[, index] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{()}
\NormalTok{        \}}
\NormalTok{        num_nas =}\StringTok{ }\NormalTok{items }\OperatorTok{%>%}\StringTok{ }\KeywordTok{is.na}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sum}\NormalTok{()}
        \ControlFlowTok{if}\NormalTok{ (cutoff }\OperatorTok{<}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
\NormalTok{            perc_nas =}\StringTok{ }\NormalTok{num_nas}\OperatorTok{/}\KeywordTok{length}\NormalTok{(items)}
            \KeywordTok{return}\NormalTok{(perc_nas }\OperatorTok{<}\StringTok{ }\NormalTok{cutoff)}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
            \KeywordTok{return}\NormalTok{(num_nas }\OperatorTok{<}\StringTok{ }\NormalTok{cutoff)}
\NormalTok{        \}}
        \KeywordTok{return}\NormalTok{(}\OtherTok{NULL}\NormalTok{)}
\NormalTok{    \}}
    
    \ControlFlowTok{if}\NormalTok{ (rows) \{}
\NormalTok{        keep_row =}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data_set), drop_index, }
\NormalTok{            data_set)}
        \KeywordTok{return}\NormalTok{(data_set[keep_row, ])}
\NormalTok{    \}}
\NormalTok{    keep_col =}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(data_set), drop_index, }
\NormalTok{        data_set)}
    \KeywordTok{return}\NormalTok{(data_set[, keep_col])}
\NormalTok{\}}

\CommentTok{# Uses knn imputation from the caret package to}
\CommentTok{# impute missing values that match the format of}
\CommentTok{# surrounding variables}
\NormalTok{full_imputation =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data_set, }\DataTypeTok{accuracy =} \OtherTok{FALSE}\NormalTok{) \{}
\NormalTok{    packages_loaded =}\StringTok{ }\KeywordTok{require}\NormalTok{(caret) }\OperatorTok{&}\StringTok{ }\KeywordTok{require}\NormalTok{(dplyr)}
    \ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\NormalTok{packages_loaded) \{}
        \KeywordTok{stop}\NormalTok{(}\StringTok{"The caret and dplyr packages are necessary for this function!"}\NormalTok{)}
\NormalTok{    \}}
    
\NormalTok{    altered_ds =}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(data_set, as_numeric)}
\NormalTok{    imputed_model =}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(altered_ds, }\StringTok{"knnImpute"}\NormalTok{)}
\NormalTok{    imputed_ds =}\StringTok{ }\NormalTok{imputed_model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{predict}\NormalTok{(altered_ds) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{unscale}\NormalTok{(altered_ds)}
    \CommentTok{# message(confusionMatrix(imputed_model))}
    \KeywordTok{message}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"Imputation accuracy with known values: "}\NormalTok{, }
        \KeywordTok{imputation_performance}\NormalTok{(altered_ds, imputed_ds) }\OperatorTok{%>%}\StringTok{ }
\StringTok{            }\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) x }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DecValTok{1}\NormalTok{), }\StringTok{"%"}\NormalTok{, }
        \DataTypeTok{sep =} \StringTok{""}\NormalTok{))}
    
\NormalTok{    replace_var_nas =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(var_name, ds) \{}
\NormalTok{        na_indices =}\StringTok{ }\NormalTok{altered_ds }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(var_name) }\OperatorTok{%>%}\StringTok{ }
\StringTok{            }\NormalTok{\{}
                \KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(.))}
\NormalTok{            \}}
        \ControlFlowTok{if}\NormalTok{ (}\OtherTok{FALSE} \OperatorTok{%in%}\StringTok{ }\KeywordTok{is.wholenumber}\NormalTok{(ds[[var_name]])) \{}
\NormalTok{            ds[[var_name]][na_indices] =}\StringTok{ }\NormalTok{imputed_ds[[var_name]][na_indices]}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{            ds[[var_name]][na_indices] =}\StringTok{ }\KeywordTok{round}\NormalTok{(imputed_ds[[var_name]][na_indices])}
\NormalTok{        \}}
\NormalTok{        ds[[var_name]] =}\StringTok{ }\KeywordTok{switch_other}\NormalTok{(}\KeywordTok{typeof}\NormalTok{(data_set[[var_name]]), }
            \DataTypeTok{character =} \KeywordTok{factor}\NormalTok{(data_set[[var_name]]) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                }\KeywordTok{levels}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\NormalTok{.[ds[[var_name]]], }\DataTypeTok{factor =} \KeywordTok{factor}\NormalTok{(data_set[[var_name]]) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                }\KeywordTok{levels}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\NormalTok{.[ds[[var_name]]] }\OperatorTok{%>%}\StringTok{ }
\StringTok{                }\KeywordTok{factor}\NormalTok{(), }\DataTypeTok{logical =} \KeywordTok{as.logical}\NormalTok{(ds[[var_name]]), }
            \DataTypeTok{other =}\NormalTok{ ds[[var_name]])}
        
        \KeywordTok{return}\NormalTok{(ds[[var_name]])}
\NormalTok{    \}}
    \KeywordTok{return}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(}\KeywordTok{names}\NormalTok{(altered_ds), replace_var_nas, }
\NormalTok{        altered_ds) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{())}
\NormalTok{\}}

\NormalTok{imputation_performance =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(original, imputed, }
    \DataTypeTok{accuracy =} \OtherTok{TRUE}\NormalTok{) \{}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{nrow}\NormalTok{(original) }\OperatorTok{!=}\StringTok{ }\KeywordTok{nrow}\NormalTok{(imputed) }\OperatorTok{|}\StringTok{ }\KeywordTok{ncol}\NormalTok{(original) }\OperatorTok{!=}\StringTok{ }
\StringTok{        }\KeywordTok{ncol}\NormalTok{(imputed)) \{}
        \KeywordTok{stop}\NormalTok{(}\StringTok{"Dimensions of objects must match!"}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{    tune_grid =}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{row_index =} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(original), }
        \DataTypeTok{col_index =} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(original))}
\NormalTok{    get_val =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(grid_index, }\DataTypeTok{acc =} \OtherTok{TRUE}\NormalTok{) \{}
\NormalTok{        indices =}\StringTok{ }\KeywordTok{unlist}\NormalTok{(tune_grid[grid_index, ])}
\NormalTok{        original_val =}\StringTok{ }\KeywordTok{unlist}\NormalTok{(original[indices[}\DecValTok{1}\NormalTok{], }
\NormalTok{            indices[}\DecValTok{2}\NormalTok{]])}
        \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{is.na}\NormalTok{(original_val)) \{}
            \KeywordTok{return}\NormalTok{(original_val)}
\NormalTok{        \}}
        \ControlFlowTok{if}\NormalTok{ (acc) \{}
            \KeywordTok{return}\NormalTok{(original_val }\OperatorTok{==}\StringTok{ }\KeywordTok{unlist}\NormalTok{(imputed[indices[}\DecValTok{1}\NormalTok{], }
\NormalTok{                indices[}\DecValTok{2}\NormalTok{]]))}
\NormalTok{        \}}
        \KeywordTok{return}\NormalTok{(}\KeywordTok{abs}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(original_val) }\OperatorTok{-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(imputed[indices[}\DecValTok{1}\NormalTok{], }
\NormalTok{            indices[}\DecValTok{2}\NormalTok{]]))))}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{ (accuracy) \{}
        \KeywordTok{return}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(tune_grid), get_val) }\OperatorTok{%>%}\StringTok{ }
\StringTok{            }\KeywordTok{as.logical}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(x))))}
\NormalTok{    \}}
    \KeywordTok{return}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(tune_grid), get_val, }\OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{as.numeric}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(x))))}
\NormalTok{\}}

\CommentTok{# Coerces vectors to numeric vectors, for character}
\CommentTok{# vectors that can not be coerced directly to}
\CommentTok{# numeric, the vector is coerced to a factor then a}
\CommentTok{# numeric vector}
\NormalTok{as_numeric =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    num_nas =}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x))}
\NormalTok{    x_num =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(x)}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x_num)) }\OperatorTok{>}\StringTok{ }\NormalTok{num_nas) \{}
\NormalTok{        x_num =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{factor}\NormalTok{(x))}
\NormalTok{    \}}
    \KeywordTok{return}\NormalTok{(x_num)}
\NormalTok{\}}

\CommentTok{# a method for reversing the scaling process}
\CommentTok{# performed by scale(), methods automatically call}
\CommentTok{# the right function for the right input class,}
\CommentTok{# when the generic method is used. This allows}
\CommentTok{# unscale() to work for numeric vectors, data}
\CommentTok{# frames, and tibbles}

\NormalTok{unscale =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(scaled, original) \{}
    \KeywordTok{UseMethod}\NormalTok{(}\StringTok{"unscale"}\NormalTok{)}
\NormalTok{\}}

\NormalTok{unscale.numeric =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(scaled, original) \{}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(scaled) }\OperatorTok{!=}\StringTok{ }\KeywordTok{length}\NormalTok{(original)) \{}
        \KeywordTok{stop}\NormalTok{(}\StringTok{"Objects must match each other in format!"}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{    orig_mean =}\StringTok{ }\KeywordTok{mean}\NormalTok{(original, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{    orig_sd =}\StringTok{ }\KeywordTok{sd}\NormalTok{(original, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
    \KeywordTok{return}\NormalTok{(scaled }\OperatorTok{*}\StringTok{ }\NormalTok{orig_sd }\OperatorTok{+}\StringTok{ }\NormalTok{orig_mean)}
\NormalTok{\}}

\NormalTok{unscale.data.frame =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(scaled, original) \{}
    \ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{identical}\NormalTok{(}\KeywordTok{names}\NormalTok{(scaled), }\KeywordTok{names}\NormalTok{(original))) \{}
        \KeywordTok{stop}\NormalTok{(}\StringTok{"Objects must match each other in format!"}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{    us_col =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(var_index) \{}
        \KeywordTok{return}\NormalTok{(}\KeywordTok{unscale.numeric}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(scaled[, var_index]), }
            \KeywordTok{unlist}\NormalTok{(original[, var_index])))}
\NormalTok{    \}}
\NormalTok{    unscaled_df =}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(scaled), }
\NormalTok{        us_col))}
    \KeywordTok{names}\NormalTok{(unscaled_df) =}\StringTok{ }\KeywordTok{names}\NormalTok{(scaled)}
    \KeywordTok{return}\NormalTok{(unscaled_df)}
\NormalTok{\}}

\NormalTok{unscale.tbl_df =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(scaled, original) \{}
    \KeywordTok{return}\NormalTok{(}\KeywordTok{as_tibble}\NormalTok{(}\KeywordTok{unscale.data.frame}\NormalTok{(scaled, original)))}
\NormalTok{\}}

\CommentTok{# from the helppage for is.integer()}
\NormalTok{is.wholenumber =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, }\DataTypeTok{tol =}\NormalTok{ .Machine}\OperatorTok{$}\NormalTok{double.eps}\OperatorTok{^}\FloatTok{0.5}\NormalTok{) }\KeywordTok{abs}\NormalTok{(x }\OperatorTok{-}\StringTok{ }
\StringTok{    }\KeywordTok{round}\NormalTok{(x)) }\OperatorTok{<}\StringTok{ }\NormalTok{tol}

\CommentTok{# a switch function that catches all 'other'}
\CommentTok{# results of the expression}
\NormalTok{switch_other =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(EXPR, ..., }\DataTypeTok{other =} \OtherTok{NULL}\NormalTok{) \{}
\NormalTok{    sw_result =}\StringTok{ }\ControlFlowTok{switch}\NormalTok{(EXPR, ...)}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{is.null}\NormalTok{(sw_result)) \{}
        \KeywordTok{return}\NormalTok{(other)}
\NormalTok{    \}}
    \KeywordTok{return}\NormalTok{(sw_result)}
\NormalTok{\}}


\CommentTok{## ----data---------------------------------------------------------------------}
\NormalTok{original_data =}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"compressed_okcupid.csv"}\NormalTok{))}

\NormalTok{new_features =}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"newfeatures.csv"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{scale}\NormalTok{()}

\NormalTok{cluster_data =}\StringTok{ }\NormalTok{original_data  }\CommentTok{#%>% select(c('age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity', 'job', 'orientation', 'religion', 'sex', 'smokes', 'status', 'sign_import', 'dogs', 'cats', 'kids', 'multi_ling', 'days_since_online', 'distance')) %>% select(-c(cats, dogs)) %>%  mutate_if(is.character, factor) %>% filter(distance < 50) %>% select(-distance) }


\CommentTok{## ----imputing, eval=FALSE, error=TRUE, cache=TRUE,}
\CommentTok{## include=FALSE-------------- if}
\CommentTok{## ('full_imputed.rds' %in%}
\CommentTok{## list.files(here('Data'))) \{ imputed =}
\CommentTok{## readRDS(here('Data', 'full_imputed.rds')) \} else}
\CommentTok{## \{ setup_cl = function(seed = round(Sys.time())) \{}
\CommentTok{## require(parallel) if (exists('cl')) \{}
\CommentTok{## print('Stopping existing cluster')}
\CommentTok{## try(parallel::stopCluster(cl)) \} assign('cl',}
\CommentTok{## parallel::makeCluster(parallel::detectCores() -}
\CommentTok{## 1, outfile = 'out.txt'), envir = globalenv())}
\CommentTok{## RNGkind('L'Ecuyer-CMRG') print(paste('Using',}
\CommentTok{## as.numeric(seed), 'as parallel RNG seed'))}
\CommentTok{## clusterSetRNGStream(cl, seed) \} setup_cl(60615)}
\CommentTok{## registerDoParallel(cl) tic() imputed =}
\CommentTok{## cluster_data %>% as.data.frame() %>%}
\CommentTok{## missForest(parallelize = 'forests') toc()}
\CommentTok{## saveRDS(imputed, here('Data',}
\CommentTok{## 'full_imputed.rds')) \} imputed_data =}
\CommentTok{## imputed$ximp imputed$OOBerror %>% kable()}


\CommentTok{## ----pca-1, eval=FALSE,}
\CommentTok{## include=FALSE-----------------------------------------}
\CommentTok{## PCA(new_features, graph = FALSE) %>%}
\CommentTok{## fviz_pca_biplot(label = 'var', col.var = 'red',}
\CommentTok{## col.ind = 'grey') ggsave2(here('Clustering',}
\CommentTok{## 'pca.png'), height = 7, width = 11)}


\CommentTok{## ----clara, eval=FALSE,}
\CommentTok{## include=FALSE-----------------------------------------}
\CommentTok{## #nb <- NbClust(new_features, distance =}
\CommentTok{## 'euclidean', min.nc = 2, max.nc = 10, method =}
\CommentTok{## 'complete', index ='all') build_clara =}
\CommentTok{## function(x) \{ clara(new_features, x) \} clara_mods}
\CommentTok{## = lapply(1:10, build_clara) clara_viz =}
\CommentTok{## lapply(clara_mods[2:10], fviz_cluster, labelsize}
\CommentTok{## = 0) plot_grid(plotlist = clara_viz)}
\CommentTok{## ggsave2(here('Clustering', 'clara_all.png'),}
\CommentTok{## height = 7, width = 11) clara_viz[[2]]}
\CommentTok{## ggsave2(here('Clustering', 'clara_three.png'),}
\CommentTok{## height = 7, width = 11)}


\CommentTok{## ----dbscan, eval=FALSE, message=FALSE,}
\CommentTok{## warning=FALSE, include=FALSE----------}
\CommentTok{## kNNdistplot(new_features, k = ncol(new_features)}
\CommentTok{## - 1) abline(h= 0.5, col = 'red', lty=2) test =}
\CommentTok{## dbscan(new_features, 0.5) fviz_cluster(test,}
\CommentTok{## new_features, labelsize = 0)}


\CommentTok{## ----descr-stats, cache=TRUE, fig.height = 8,}
\CommentTok{## fig.width = 11------------------}
\NormalTok{skim_list =}\StringTok{ }\NormalTok{original_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{skim}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{partition}\NormalTok{()}

\NormalTok{skim_list}\OperatorTok{$}\NormalTok{numeric }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{()}
\NormalTok{skim_list}\OperatorTok{$}\NormalTok{character }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{()}

\NormalTok{original_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{essay0, }\OperatorTok{-}\NormalTok{essay9) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.character, }
\NormalTok{    factor) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(as.numeric) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{cor}\NormalTok{(}\DataTypeTok{use =} \StringTok{"pairwise.complete.obs"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{ggcorrplot}\NormalTok{()}

\NormalTok{clusterability =}\StringTok{ }\NormalTok{original_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{essay0, }
    \OperatorTok{-}\NormalTok{essay9) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.character, factor) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_all}\NormalTok{(as.numeric) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{5000}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{get_clust_tendency}\NormalTok{(}\DataTypeTok{n =} \DecValTok{50}\NormalTok{)}
\NormalTok{clusterability}\OperatorTok{$}\NormalTok{hopkins_stat}
\NormalTok{clusterability}\OperatorTok{$}\NormalTok{plot}


\CommentTok{## ----pca,}
\CommentTok{## cache=TRUE----------------------------------------------------------}
\NormalTok{sampled_data =}\StringTok{ }\NormalTok{original_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{2000}\NormalTok{)}
\NormalTok{sampled_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{essay0, }\OperatorTok{-}\NormalTok{essay9) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.character, }
\NormalTok{    factor) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(as.numeric) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{PCA}\NormalTok{(}\DataTypeTok{graph =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{fviz_pca_biplot}\NormalTok{(}\DataTypeTok{label =} \StringTok{"var"}\NormalTok{, }\DataTypeTok{col.var =} \StringTok{"red"}\NormalTok{, }
        \DataTypeTok{col.ind =} \StringTok{"grey"}\NormalTok{)}
\KeywordTok{ggsave2}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Clustering"}\NormalTok{, }\StringTok{"pca_v2.png"}\NormalTok{), }\DataTypeTok{height =} \DecValTok{7}\NormalTok{, }
    \DataTypeTok{width =} \DecValTok{11}\NormalTok{)}


\CommentTok{## ----pam, eval=FALSE,}
\CommentTok{## include=FALSE-------------------------------------------}
\CommentTok{## gower_data = original_data %>% mutate_all(factor)}
\CommentTok{## %>% daisy(metric = 'gower') #gower_data %>%}
\CommentTok{## fviz_dist() sil_width <- c(NA) for(i in 2:15)\{}
\CommentTok{## pam_fit <- pam(gower_data, diss = TRUE, k = i)}
\CommentTok{## sil_width[i] <- pam_fit$silinfo$avg.width \}}
\CommentTok{## plot(1:8, sil_width, xlab = 'Number of clusters',}
\CommentTok{## ylab = 'Silhouette Width') lines(1:8, sil_width)}


\CommentTok{## ----agg-nest,}
\CommentTok{## error=TRUE-----------------------------------------------------}
\NormalTok{agnes_data =}\StringTok{ }\NormalTok{sampled_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{essay0, }\OperatorTok{-}\NormalTok{essay9) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.character, factor) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_all}\NormalTok{(as.numeric) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_all}\NormalTok{(scale)}
\NormalTok{agnes_diss =}\StringTok{ }\NormalTok{agnes_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{daisy}\NormalTok{(}\DataTypeTok{metric =} \StringTok{"gower"}\NormalTok{)}
\NormalTok{nb_results =}\StringTok{ }\KeywordTok{NbClust}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ agnes_data, }\DataTypeTok{diss =}\NormalTok{ agnes_diss, }
    \DataTypeTok{distance =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{min.nc =} \DecValTok{2}\NormalTok{, }\DataTypeTok{max.nc =} \DecValTok{10}\NormalTok{, }\DataTypeTok{method =} \StringTok{"ward.D2"}\NormalTok{)}

\KeywordTok{fviz_nbclust}\NormalTok{(nb_results)}

\NormalTok{agnes_mod =}\StringTok{ }\NormalTok{agnes_diss }\OperatorTok{%>%}\StringTok{ }\KeywordTok{hcut}\NormalTok{(}\DataTypeTok{isdiss =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{k =} \DecValTok{3}\NormalTok{, }
    \DataTypeTok{hc_func =} \StringTok{"agnes"}\NormalTok{, }\DataTypeTok{hc_method =} \StringTok{"ward.D2"}\NormalTok{)}
\KeywordTok{fviz_dend}\NormalTok{(agnes_mod)}
\NormalTok{sampled_data}\OperatorTok{$}\NormalTok{cluster =}\StringTok{ }\NormalTok{agnes_mod}\OperatorTok{$}\NormalTok{cluster}
\KeywordTok{fviz_cluster}\NormalTok{(agnes_mod, }\DataTypeTok{data =}\NormalTok{ agnes_diss, }\DataTypeTok{labelsize =} \DecValTok{0}\NormalTok{)}

\KeywordTok{saveRDS}\NormalTok{(sampled_data, }\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Results"}\NormalTok{, }\StringTok{"agnes_results.rds"}\NormalTok{))}
\KeywordTok{write_csv}\NormalTok{(sampled_data, }\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Results"}\NormalTok{, }\StringTok{"agnes_results.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{dbscan-code}{%
\subsection{DBSCAN Code}\label{dbscan-code}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## ----setup,}
\CommentTok{## include=FALSE-----------------------------------------------------}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(cluster)}
\KeywordTok{library}\NormalTok{(FactoMineR)}
\KeywordTok{library}\NormalTok{(factoextra)}
\KeywordTok{library}\NormalTok{(NbClust)}
\KeywordTok{library}\NormalTok{(dbscan)}
\KeywordTok{library}\NormalTok{(cowplot)}
\KeywordTok{library}\NormalTok{(skimr)}
\KeywordTok{library}\NormalTok{(ggcorrplot)}

\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{fig.height =} \DecValTok{6}\NormalTok{, }
    \DataTypeTok{fig.width =} \DecValTok{8}\NormalTok{, }\DataTypeTok{dpi =} \DecValTok{400}\NormalTok{)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{60615}\NormalTok{)}


\CommentTok{## ----data---------------------------------------------------------------------}
\NormalTok{demo_data =}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"compressed_okcupid.csv"}\NormalTok{))}
\NormalTok{doc2vec_data =}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"doc2vec_results.csv"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{bind_cols}\NormalTok{(}\KeywordTok{select}\NormalTok{(demo_data, long_words, flesch)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{scale}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{()}
\NormalTok{doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{skim}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{partition}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\NormalTok{.}\OperatorTok{$}\NormalTok{numeric }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{kable}\NormalTok{()}
\NormalTok{doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\NormalTok{\{}
    \KeywordTok{ggcorrplot}\NormalTok{(}\KeywordTok{cor}\NormalTok{(.), }\DataTypeTok{p.mat =} \KeywordTok{cor_pmat}\NormalTok{(.), }\DataTypeTok{hc.order =} \OtherTok{TRUE}\NormalTok{, }
        \DataTypeTok{insig =} \StringTok{"blank"}\NormalTok{)}
\NormalTok{\}}


\CommentTok{## ----clusterability-----------------------------------------------------------}
\NormalTok{clusterability =}\StringTok{ }\NormalTok{doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{get_clust_tendency}\NormalTok{(}\DataTypeTok{n =} \DecValTok{15}\NormalTok{)}
\CommentTok{# clusterability$plot}


\CommentTok{## ----dbscan-------------------------------------------------------------------}
\NormalTok{doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kNNdistplot}\NormalTok{(}\DataTypeTok{k =} \DecValTok{5}\NormalTok{)}
\NormalTok{dbscan_mod =}\StringTok{ }\NormalTok{doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dbscan}\NormalTok{(}\DecValTok{9}\NormalTok{, }
    \DecValTok{5}\NormalTok{)}
\NormalTok{doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\NormalTok{\{}
    \KeywordTok{fviz_cluster}\NormalTok{(dbscan_mod, }\DataTypeTok{data =}\NormalTok{ .)}
\NormalTok{\}}

\NormalTok{dbscan_results =}\StringTok{ }\NormalTok{doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{bind_cols}\NormalTok{(}\KeywordTok{enframe}\NormalTok{(dbscan_mod}\OperatorTok{$}\NormalTok{cluster, }
    \DataTypeTok{name =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{value =} \StringTok{"cluster"}\NormalTok{))}
\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"compressed_with_results.csv"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{dbscan_cluster =}\NormalTok{ dbscan_mod}\OperatorTok{$}\NormalTok{cluster) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{write_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"compressed_with_results.csv"}\NormalTok{))}


\CommentTok{## ----merge--------------------------------------------------------------------}
\NormalTok{merged_demo_data =}\StringTok{ }\NormalTok{demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{essay0, }\OperatorTok{-}\NormalTok{essay9) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{bind_cols}\NormalTok{(}\KeywordTok{select}\NormalTok{(dbscan_results, cluster))}
\NormalTok{merged_doc2vec_data =}\StringTok{ }\NormalTok{demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(X1, essay0, }
\NormalTok{    essay9) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{bind_cols}\NormalTok{(}\KeywordTok{select}\NormalTok{(dbscan_results, }\OperatorTok{-}\NormalTok{X1))}


\CommentTok{## ----demo-data----------------------------------------------------------------}
\NormalTok{modal =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(vect, }\DataTypeTok{percent =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{only_one =} \OtherTok{FALSE}\NormalTok{) \{}
    \KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{    modal_val =}\StringTok{ }\NormalTok{vect }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{table}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\NormalTok{.[. }\OperatorTok{==}\StringTok{ }
\StringTok{        }\KeywordTok{max}\NormalTok{(.)] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{names}\NormalTok{()}
    \ControlFlowTok{if}\NormalTok{ (only_one) \{}
\NormalTok{        modal_val =}\StringTok{ }\NormalTok{modal_val[}\DecValTok{1}\NormalTok{]}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{ (percent) \{}
        \KeywordTok{return}\NormalTok{(vect }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\NormalTok{.[. }\OperatorTok{==}\StringTok{ }\NormalTok{modal_val] }\OperatorTok{%>%}\StringTok{ }
\StringTok{            }\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{length}\NormalTok{(x)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(vect)))}
\NormalTok{    \}}
\NormalTok{    modal_val}
\NormalTok{\}}

\NormalTok{cluster_significance =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(var, data, }\DataTypeTok{clus_var =} \StringTok{"cluster"}\NormalTok{) \{}
    \KeywordTok{wilcox.test}\NormalTok{(}\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(var, }\StringTok{"~"}\NormalTok{, clus_var)), }
\NormalTok{        data)}\OperatorTok{$}\NormalTok{p.value}
\NormalTok{\}}

\NormalTok{merged_demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(X1, education)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise_all}\NormalTok{(mean) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Mean by Cluster"}\NormalTok{)}

\NormalTok{merged_demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\NormalTok{\{}
        \KeywordTok{sapply}\NormalTok{(}\KeywordTok{names}\NormalTok{(}\KeywordTok{select}\NormalTok{(., }\OperatorTok{-}\NormalTok{cluster)), cluster_significance, }
            \DataTypeTok{data =}\NormalTok{ .)}
\NormalTok{    \} }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{enframe}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Wilcox Test P-values"}\NormalTok{)}

\NormalTok{merged_demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(X1, education)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise_all}\NormalTok{(sd) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Standard Deviation by Cluster"}\NormalTok{)}
\NormalTok{merged_demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(X1, education)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise_all}\NormalTok{(median) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Median by Cluster"}\NormalTok{)}

\NormalTok{merged_demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(X1, education)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{cluster =} \KeywordTok{factor}\NormalTok{(cluster)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select_if}\NormalTok{((}\ControlFlowTok{function}\NormalTok{(x) }\OperatorTok{!}\KeywordTok{is.numeric}\NormalTok{(x))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise_all}\NormalTok{(modal, }\DataTypeTok{only_one =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Mode by Cluster"}\NormalTok{)}
\NormalTok{merged_demo_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(X1, education)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{cluster =} \KeywordTok{factor}\NormalTok{(cluster)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select_if}\NormalTok{((}\ControlFlowTok{function}\NormalTok{(x) }\OperatorTok{!}\KeywordTok{is.numeric}\NormalTok{(x))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarise_all}\NormalTok{(modal, }\DataTypeTok{percent =} \OtherTok{TRUE}\NormalTok{, }
    \DataTypeTok{only_one =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.numeric, round, }
    \DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Mode by Cluster"}\NormalTok{)}


\CommentTok{## ----interpret-outliers-------------------------------------------------------}
\NormalTok{dbscan_results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise_all}\NormalTok{(mean) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Mean by Cluster"}\NormalTok{)}
\NormalTok{dbscan_results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise_all}\NormalTok{(sd) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Standard Deviation by Cluster"}\NormalTok{)}
\NormalTok{dbscan_results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{X1) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise_all}\NormalTok{(median) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Median by Cluster"}\NormalTok{)}


\CommentTok{## ----profile-diff-------------------------------------------------------------}
\NormalTok{merged_doc2vec_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(cluster, essay0) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{group_by}\NormalTok{(cluster) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{sample_n}\NormalTok{(}\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{doc2vec-code}{%
\subsection{Doc2Vec Code}\label{doc2vec-code}}

\hypertarget{structural-topic-modeling-code}{%
\subsection{Structural topic modeling
Code}\label{structural-topic-modeling-code}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## ----setup,}
\CommentTok{## include=FALSE-----------------------------------------------------}
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{library}\NormalTok{(stm)}
\KeywordTok{library}\NormalTok{(quanteda)}
\KeywordTok{library}\NormalTok{(topicmodels)}
\KeywordTok{library}\NormalTok{(tidytext)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(tidyr)}
\KeywordTok{library}\NormalTok{(scales)}
\KeywordTok{library}\NormalTok{(tm)}
\KeywordTok{library}\NormalTok{(grid)}
\KeywordTok{library}\NormalTok{(wordcloud)}
\KeywordTok{library}\NormalTok{(wordcloud2)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(igraph)}
\KeywordTok{library}\NormalTok{(stmCorrViz)}


\CommentTok{## ----data,}
\CommentTok{## echo=FALSE---------------------------------------------------------}
\KeywordTok{setwd}\NormalTok{(}\StringTok{"C:/Users/lliu9/Desktop/UML_Project/unsupervised-dating"}\NormalTok{)}
\CommentTok{# cleaned okcupid data}
\NormalTok{data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"final_okcupid.csv"}\NormalTok{))}

\CommentTok{# data <- sample_n(essay, 100)}
\NormalTok{data <-}\StringTok{ }\NormalTok{data[}\KeywordTok{c}\NormalTok{(}\StringTok{"clean_text"}\NormalTok{, }\StringTok{"fit"}\NormalTok{, }\StringTok{"edu"}\NormalTok{, }\StringTok{"height_group"}\NormalTok{, }
    \StringTok{"race_ethnicity"}\NormalTok{, }\StringTok{"dbscan_cluster"}\NormalTok{)]}


\CommentTok{## ----clean--------------------------------------------------------------------}
\NormalTok{processed <-}\StringTok{ }\KeywordTok{textProcessor}\NormalTok{(data}\OperatorTok{$}\NormalTok{clean_text, }\DataTypeTok{metadata =}\NormalTok{ data)}
\NormalTok{out <-}\StringTok{ }\KeywordTok{prepDocuments}\NormalTok{(processed}\OperatorTok{$}\NormalTok{documents, processed}\OperatorTok{$}\NormalTok{vocab, }
\NormalTok{    processed}\OperatorTok{$}\NormalTok{meta, }\DataTypeTok{lower.thresh =} \DecValTok{0}\NormalTok{)}
\NormalTok{docs <-}\StringTok{ }\NormalTok{out}\OperatorTok{$}\NormalTok{documents}
\NormalTok{vocab <-}\StringTok{ }\NormalTok{out}\OperatorTok{$}\NormalTok{vocab}
\NormalTok{meta <-}\StringTok{ }\NormalTok{out}\OperatorTok{$}\NormalTok{meta}


\CommentTok{## ----stm----------------------------------------------------------------------}
\NormalTok{Fit <-}\StringTok{ }\KeywordTok{stm}\NormalTok{(}\DataTypeTok{documents =}\NormalTok{ out}\OperatorTok{$}\NormalTok{documents, }\DataTypeTok{vocab =}\NormalTok{ out}\OperatorTok{$}\NormalTok{vocab, }
    \DataTypeTok{K =} \DecValTok{9}\NormalTok{, }\DataTypeTok{prevalence =} \OperatorTok{~}\NormalTok{fit }\OperatorTok{+}\StringTok{ }\NormalTok{edu }\OperatorTok{+}\StringTok{ }\NormalTok{height_group }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{race_ethnicity }\OperatorTok{+}\StringTok{ }\NormalTok{dbscan_cluster, }\DataTypeTok{max.em.its =} \DecValTok{50}\NormalTok{, }
    \DataTypeTok{data =}\NormalTok{ out}\OperatorTok{$}\NormalTok{meta, }\DataTypeTok{init.type =} \StringTok{"Spectral"}\NormalTok{, }\DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{)}


\CommentTok{## ----label--------------------------------------------------------------------}
\KeywordTok{labelTopics}\NormalTok{(Fit, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{))}


\CommentTok{## ----estimate-----------------------------------------------------------------}
\NormalTok{prep <-}\StringTok{ }\KeywordTok{estimateEffect}\NormalTok{(}\OperatorTok{~}\NormalTok{fit }\OperatorTok{+}\StringTok{ }\NormalTok{edu }\OperatorTok{+}\StringTok{ }\NormalTok{height_group }\OperatorTok{+}\StringTok{ }
\StringTok{    }\NormalTok{race_ethnicity }\OperatorTok{+}\StringTok{ }\NormalTok{dbscan_cluster, Fit, }\DataTypeTok{meta =}\NormalTok{ out}\OperatorTok{$}\NormalTok{meta, }
    \DataTypeTok{uncertainty =} \StringTok{"Global"}\NormalTok{)}


\CommentTok{## ----print--------------------------------------------------------------------}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{)) \{}
    \KeywordTok{print}\NormalTok{(}\KeywordTok{summary}\NormalTok{(prep, }\DataTypeTok{topic =}\NormalTok{ i))}
\NormalTok{\}}


\CommentTok{## ----summary------------------------------------------------------------------}
\CommentTok{## jpeg('toptopics.jpeg')}
\KeywordTok{plot}\NormalTok{(Fit, }\DataTypeTok{type =} \StringTok{"summary"}\NormalTok{, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.3}\NormalTok{))}
\CommentTok{# dev.off}


\CommentTok{## ----labels-------------------------------------------------------------------}
\KeywordTok{jpeg}\NormalTok{(}\StringTok{"top3topics.jpeg"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(Fit, }\DataTypeTok{type =} \StringTok{"labels"}\NormalTok{, }\DataTypeTok{topics =} \KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\KeywordTok{dev.off}\NormalTok{()}


\CommentTok{## ----labels2------------------------------------------------------------------}
\KeywordTok{jpeg}\NormalTok{(}\StringTok{"bottom3topics.jpeg"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(Fit, }\DataTypeTok{type =} \StringTok{"labels"}\NormalTok{, }\DataTypeTok{topics =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{9}\NormalTok{))}
\KeywordTok{dev.off}\NormalTok{()}


\CommentTok{## ----hist---------------------------------------------------------------------}
\KeywordTok{jpeg}\NormalTok{(}\StringTok{"hist.jpeg"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(Fit, }\DataTypeTok{type =} \StringTok{"hist"}\NormalTok{)}
\KeywordTok{dev.off}\NormalTok{()}


\CommentTok{## ----perspectives-------------------------------------------------------------}
\KeywordTok{jpeg}\NormalTok{(}\StringTok{"comparison.jpeg"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(Fit, }\DataTypeTok{type =} \StringTok{"perspectives"}\NormalTok{, }\DataTypeTok{topics =} \KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\KeywordTok{dev.off}\NormalTok{()}



\CommentTok{## ----quality------------------------------------------------------------------}
\KeywordTok{jpeg}\NormalTok{(}\StringTok{"topicQuality.jpeg"}\NormalTok{)}
\KeywordTok{topicQuality}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ Fit, }\DataTypeTok{documents =}\NormalTok{ docs)}
\KeywordTok{dev.off}\NormalTok{()}


\CommentTok{## ----cloud--------------------------------------------------------------------}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{)) \{}
    \CommentTok{# name <- paste('topic', str(i), '.jpg', sep='')}
    \CommentTok{# jpeg(name, width = 350, height = '350')}
    \KeywordTok{cloud}\NormalTok{(Fit, }\DataTypeTok{topic =}\NormalTok{ i, }\DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\NormalTok{\}}


\CommentTok{## ----network------------------------------------------------------------------}
\CommentTok{## Positive correlations between topics indicate}
\CommentTok{## that both topics are likely to be discussed}
\CommentTok{## within a document.}
\NormalTok{mod.out.corr <-}\StringTok{ }\KeywordTok{topicCorr}\NormalTok{(Fit)}
\CommentTok{# Graphical display of topic correlations.}
\KeywordTok{jpeg}\NormalTok{(}\StringTok{"network.jpeg"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(mod.out.corr)}
\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-bruning1999cognitive}{}%
Bruning, Roger H, Gregory J Schraw, and Royce R Ronning. 1999.
\emph{Cognitive Psychology and Instruction}. ERIC.

\leavevmode\hypertarget{ref-cacioppo2013marital}{}%
Cacioppo, John T, Stephanie Cacioppo, Gian C Gonzaga, Elizabeth L
Ogburn, and Tyler J VanderWeele. 2013. ``Marital Satisfaction and
Break-Ups Differ Across on-Line and Off-Line Meeting Venues.''
\emph{Proceedings of the National Academy of Sciences} 110 (25):
10135--40.

\leavevmode\hypertarget{ref-goffman1975presentation}{}%
Goffman, E. 1975. \emph{The Presentation of Self in Everyday Life}.
Pelican Book. Penguin Books.
\url{https://books.google.com/books?id=tYvNnQEACAAJ}.

\leavevmode\hypertarget{ref-heino2010relationshopping}{}%
Heino, Rebecca D, Nicole B Ellison, and Jennifer L Gibbs. 2010.
``Relationshopping: Investigating the Market Metaphor in Online
Dating.'' \emph{Journal of Social and Personal Relationships} 27 (4):
427--47.

\leavevmode\hypertarget{ref-hitsch2010matching}{}%
Hitsch, Gunter J, Ali Hortaçsu, and Dan Ariely. 2010. ``Matching and
Sorting in Online Dating.'' \emph{American Economic Review} 100 (1):
130--63.

\leavevmode\hypertarget{ref-kim2015okcupid}{}%
Kim, Albert Y, and Adriana Escobedo-Land. 2015. ``OkCupid Data for
Introductory Statistics and Data Science Courses.'' \emph{Journal of
Statistics Education} 23 (2).

\leavevmode\hypertarget{ref-papacharissi2010networked}{}%
Papacharissi, Zizi. 2010. \emph{A Networked Self: Identity, Community,
and Culture on Social Network Sites}. Routledge.

\end{document}
