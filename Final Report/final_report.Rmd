---
title: "Final Report"
author: "Li Liu, Abhishek Pandit, Adam Shelton"
date: "12/7/2019"
header-includes:
    - \usepackage{setspace}\doublespacing
output: 
  pdf_document:
    toc: true
    toc_depth: 1
bibliography: sources.bib
---

```{r}
install.packages('factoextra')
```


```{r main-setup, include=FALSE}
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(skimr)
library(ggcorrplot)
library(treemapify)
library(factoextra)



#set working directory
#setwd('C:/Users/lliu9/Desktop/UML_Project/unsupervised-dating/Final Report')

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, error = FALSE, message = FALSE , dpi = 400, tidy.opts=list(width.cutoff=50), tidy=TRUE)

# converts a Jupyter notebook to a regular old Python script
flatten_jupyter = function(path_to_notebook) {
  output_path = str_replace(path_to_notebook, ".ipynb", ".py")
  system(paste("python3", "-m", "nbconvert", "--to", "script", paste0("\"", path_to_notebook, "\"")))
  return(output_path)
}
```

# Contributions
```{r contrib, echo=FALSE}
tibble(Liu = c("KMeans", "Structural Topic Modeling", "Volunteer for the App Demo"), Pandit = c("Overall Strategy", "Data Cleaning", "Topic Modeling Selection & Doc2Vec Model"), Shelton = c("EDA", "AGNES", "DBSCAN")) %>% kable() %>% kable_styling() %>% row_spec(0, bold = TRUE)
```

\newpage
# Introduction

“"So tell me about yourself!"’’ 

This seemingly straightforward question in day-to-day interactions is usually met with silence and hesitation. That can no longer be the case for the 1.67 trillion online dating industry, which has grown exponentially in popularity over the last decade. Various dating apps, such as OkCupid and Coffee Meets Bagel, are designed to help the singles ‘get to know’ other people for short or long-term romantic relationships. In order to be popular and memorable, users usually have to write a short introduction to advertise themselves. Such activity could be regarded as self-marketing. As the users of dating apps come from diverse backgrounds, we are interested in how users from distinct backgrounds take different approaches to make themselves more memorable. Moreover, we design the framework of scoring users’ self-introduction and algorithm for providing writing tips (such as words for being memorable). Although our project is still preliminary, it has gained a lot of interest among our friends who struggle to find a date online. Also, our methods and analysis have the potential to be adopted by the dating website to improve the users’ experience and better achieve their mission as matchmakers. 

# Literature Review

Self-concept and self-representation have long served as grounds of debate in cognitive and positive psychology [@bruning1999cognitive] as well as social anthropology [@goffman1975presentation]. The recent spread of social networking and its specific affordances have allowed individuals to build different online ‘selves’ [@papacharissi2010networked]. One such critical scenario may be that of mate selection, which several economists and sociologists have likened this to ‘marriage marketplace’ [@hitsch2010matching]. Several online dating service providers in developed countries may facilitate the expansion of potential mates beyond the limits of even extended offline social networks @cacioppo2013marital assert that as many as one in three marriages in the United States is facilitated through these portals. @heino2010relationshopping argue that these avenues further entrench the economic dimension through an acute, implicit awareness of ‘relationshopping’. Herein, potential partners are reduced to entries in a catalog to be scrolled through. In this sense, they suggest an emerging conscientiousness of ‘marketing’, with the product being themselves, and the potential mate assuming the role of a buyer (ibid). This perception thus links the private worlds of romantic intimacy with those of mass consumption and broader perceived appeal to the opposite sex. 

Potentially, we will also use some marketing theories to understand our findings. In this economic paradigm with a very large number of identical products, the dynamics of the online dating market may resemble models of perfect competition. In such a situation, any product (in this case, person) is perfectly substituable by another. Quick swiping technology only exacerbates this tendency. The only widespread alternative from standard microeconomic theory, is that of monopolistic competition. Here, each product would expend some costs in differentiating themselves from competitors.(Varian, 2014) Varian, Hal R. Intermediate microeconomics with calculus: a modern approach. WW Norton & Company, 2014.), which can involve building distinctive brands. Building brand awareness, in turn requires inter alia- targeting specific niches and adding visual, emotional or linguistic cues that are memorable for later recall. 

This could be applied to understand online dating. Let us imagine your future mate uses the filter to narrow down the consideration sets. He/She might still face many similar choices with high matching scores to choose from. If you want to stand out from the pool, you must make yourself memorable by highlighting the uniqueness. Thus, one possible idea in this project is to explore and understand how users could increase their brand awareness and differentiate themselves in their segments

# Empirical Strategy

This study will leverage publicly open and anonymized user profile data for 59,946 users of OkCupid within a 25 mile radius of San Francisco that were extracted with permission (Kim & Escobedo-Land, 2012). These will then be harnessed to address questions of self-representation in the essay section specifically for male users. To ensure that these were active users, profiles were accepted into dataset only if they had been members as of 06/26/2012,  had been active in the previous year, and had at least one photo in their profile.  The data set includes "typical user information, lifestyle variables, and text responses to 10 essay questions. After their submission, the data were then scraped but with anonymization and removal of personally identifiable information for legal and privacy concerns. We dropped all female users, and male users with missing entries for any of the 4 key variables or essays. This reduced the total sample size to 18330. 

Like with several products and services, the user (who is the product in this case) must gain more clarity on three key dimensions of this dating marketplace:
1) Who his closest competitors are: We will aim to develop 'clusters' of similar users. 
The clustering could occur either on:
i) Demographic Variables- Check on categorical variables using Gower's distance
ii) Text Variables- First by building a Doc2Vec model
In either case, a Principal Components Analysis will most likely be necessary for dimensionality reduction and being able to visually verify the presence of clusters. 

2) What strategies are the competitors applying- In the absence of photos, the core 'strategies' available are twofold- 
a) What other men talk about- the content of their essay:
In terms of our machine learning toolbox, the best approach appears to be that of Topic Modelling. Such an exploration would look for the topics themselves, and what proportions of user profiles are devoted to different topics

The profile may also benefit in terms of memorability from low frequency words, which a Topic Model would typically ignore
b) How they take about their content:
This includes a number of aspects, including, but not limited:
i) Complexity of the Language (measurable by Flesch Index)
ii) Vocabulary Level (measurable by proportion of long words- with at least 3 syllables)
iii) Humour- we will need to develop a proxy for this variable. 
iv) Length of the profile- this is directly measurable by the number of words

3) How to stand out from the competition
This would require a clear understanding of the previous steps, as well as a similarity score. Such a score could be built using:
a) cosine Similarity in the DOc2Vec Model(purely text), which could then be subsetted using demographic information represented by the cluster
b) Check for topic distributions in their demographic and recommend deviations. This would be enabled (at least in part) by Structural Topic Modelling

As would evident from above, there are 4 major methods we will be harnessing:
1) Doc2Vec, Clustering and Topic Modeling on Text Variables
2) Clustering on Demographic Variables
3) Combining Demographic and Text Variables through Structural Topic Models

In particular, we will use several key demographic variables and the self-introduction text. Also, we currently focus on only male users.This is for two reasons:
1) Women in the US typically accord 50% higher importance to their suitors' descriptions relative to men (who place greater weight on the photos) (Emory, 2017)
2) Most American dating sites (with a few exceptions) feature a higher proportion of male users, thus resulting in higher competition (Thottam, 2019)

Our preparations proceed in three steps:

First, we fine tune our demographic variables. Besides the choice of 'male' for gender (and 'straight' for orientation, since our focus is on heterosexual users), 12 other variables with multiple levels are available. Choice will need to be guided by theory. Our key statistical assumption is that difference in the text are CAUSED by differences in demographics. In this sense, we would seek to consider those demographics whose values may lead men to alter their choice of language. Through research, we found these to be:

1) Education: Through perceived improvements in earning power as well as use of language (Torkey, 2018), (Fiore et al, 2015), (Stevens & Schaefer, 1990)
2) Height: Shepperd & Strattman (1989) OkCupid (2010)- More biological preference for taller men. Men also overstate their height on average. Using the 1st quartile as a cutoff, we split this group into short and not short. 
3) Race:  Lindqvist and Lin, 2010- Online preferences veer towards homophily within races
4) Fitness Level: Burke, 2019- Based on their choice of photos, it seems that men (if not the women themselves) consider physique to be an importnant determinant of attractiveness


Thottam, Isabel '10 Online Dating Statistics You Should Know;, E-harmony.com. Retrieved from: https://www.eharmony.com/online-dating-statistics/
Emory, Leah(2017), Bustle. 'How Many People Who Meet On Dating Apps Get Married? Swiping Isn't Just For Hookups', Retrieved from - https://www.bustle.com/p/how-many-people-who-meet-on-dating-apps-get-married-swiping-isnt-just-for-hookups-44359


Secondly, we use topic modeling (LDA) to visualize the latent topics behind the text. We also compare the differences in topic proportions for different groups and clusters of users. We attempt this with both Non-Negative Matrix Factorization (NMF) and Latent Dirichlet Allocation, given its connections with other tools such as Structural Topic Models

Thirdly, we use the structural topic modeling to estimate the impact of the demographic variables and the humor measure (DBScan on Doc2Vec vectors) on the topic proportions. 

Lastly, we discuss the potential use of our analysis and also acknowledge the challenges and limitations we face at this stage.

# Analysis & Results

## Exploratory Data Analysis

### Descriptive Statistics

```{r descr-stats-demo, echo=FALSE, cache=TRUE}
original_data = read_csv(here("Data", "final_okcupid.csv"))
skim_list = original_data %>% select(-c(dbscan_cluster, new_index, orig_index)) %>% skim() %>% partition()

skim_list$numeric %>% select(-hist) %>% mutate_if(is.numeric, round, digits = 2) %>% kable(caption = "Continuous Variables")
skim_list$character%>% kable(caption = "Other Variables")

original_data %>% select(-c(orig_index, new_index, clean_text, essay9, dbscan_cluster)) %>% select_if(is.numeric) %>% cor(use = "pairwise.complete.obs") %>% ggcorrplot() + labs(title = "Correlation Plot of Demographic Variables")
```

The data we used was approximately 60,000 anonymous OkCupid profiles from 2012 that were gathered with consent from users in the San Francisco area [@kim2015okcupid]. This data was downloaded from the GitHub page for @kim2015okcupid, [https://github.com/rudeboybert/JSE_OkCupid](https://github.com/rudeboybert/JSE_OkCupid). The data contains demographic attributes of users that were submitted to their profile, including variables like age, height, race, and education, in addition to a selection of ten short essays that users have written in response to different prompts to display on their profiles. We subsetted this data to `r nrow(original_data)` profiles of men, and generated additional features for the numbers and proportions of long words and Flesch–Kincaid readability scores of the main profile essay. The majority of male users in our sample or white, fit, not-short, and have more than a high school education. The mean reported age is 32 and the mean reported height is 70.5 inches (approximately 5 foot 9 inches).

The majority of the variables included in the demographic data are independent, but some weaker correlations do exist. As expected there are positive correlations between all the features generated from the essay text. Age is also positively correlated with our text-generated features, perhaps suggesting that older people are more educated and write with more complexity. While Flesch scores and the amount of long words are correlated, there do not appear to be any demographic interactions with that relationship.

```{r viz-analysis, echo=FALSE, cache=TRUE}
original_data %>% 
  select(edu, fit, height_group, race_ethnicity) %>% 
  mutate_all(factor) %>% 
  pivot_longer(dplyr::everything()) %>% 
  table() %>% 
  as_tibble() %>% 
  ggplot(aes(area = n, fill = value, label = value)) + 
  geom_treemap() + 
  geom_treemap_text(color = "white", place = "centre", grow = TRUE) + 
  facet_wrap(~ name) + theme(legend.position = "none") + 
  labs(title = "Categorical Variable Distributions")

original_data %>% select(-new_index, -orig_index, -age, -height, -clean_text, -essay9, -dbscan_cluster, -profile_length, -prop_longwords) %>% pivot_longer(-c(flesch, long_words)) %>% ggplot(aes(x = log(flesch + abs(min(flesch)) + 1), y = long_words, color = value)) + geom_point(alpha = 0.3, size = 1) + facet_wrap(~ name) + theme(legend.position = "none") + labs(title = "Flesch score vs. Long words by Variable", x = "Flesch Score (log)", y = "Number of Long Words")
```


### Clusterability

```{r demo-clusterability, cache=TRUE}
clusterability = original_data %>% select(-c(new_index, orig_index, clean_text, essay9, dbscan_cluster)) %>% mutate_if(is.character, factor) %>% mutate_all(as.numeric) %>% sample_n(2000) %>% scale() %>% get_clust_tendency(n = 10)
clusterability$plot
```
The demographic data is highly clusterable with a Hopkins Statistic of `r round(clusterability$hopkins, 3)` on a random subset of 2,000 observations. Unfortuantely, clusterability could not be determined for the whole dataset due to performance limitations. However, 2,000 observations should be sufficient for determining clusterability. 

## Clustering of Demographic Data

### K-means

Kmeans is a type of partitioning clustering algorithm to maximize intra-cluster homogeneity and maximize inter-cluster heterogeneity. We use four demographic variables: 'fit', 'education', 'height_group', 'race_ethnicity' as previous research shows evidence these four variables are associated with users' different behavior in online self-introductions. We use Gower's distance as these variables are categorical. We start with 3 clusters and plot with the `fviz_cluster` function.

![Kmeans with 3 clusters](Plots/kmeans3.png)

Then we find the optimal number of clusters by comparing the metric of average silhouette width. However, the metric doesn't converge and keeps growing with more clusters. As a result, kmeans is not the best for finding the optimal demographic clusters.

### AGNES

Agglomerative Nesting was used to cluster the demographic data with a bottom-up approach to contrast the K-means clustering. As an AGNES model would not complete on the full data-set, a random subset of 5,000 observations was used instead. This AGNES model used a Gower's dissimilarity matrix of the data to aid in the clustering of categorical variables, since most of the demographic variables are categorical. Using the `NbClust` package in R, we determined that the optimal number of clusters was two, when using the `ward.D2` method to match our AGNES model. The model gives us two well-defined clusters along the first two principal components. The defining factor between these two clusters, majorly appears to be height, which, per the Wilcox Test, has a statistically significant difference between the means of the two clusters by about five inches.

![NbClust Results](../Clustering/advanced_clustering_files/figure-gfm/agg-nest-3.png)

![AGNES Dendrogram](../Clustering/advanced_clustering_files/figure-gfm/agg-nest-4.png)

![AGNES Cluster Results](../Clustering/advanced_clustering_files/figure-gfm/agg-nest-5.png)

## Text Analysis

### Word2Vec and Doc2Vec

Word2Vec is a two-layer neural network that assigns scores to a word in high-dimensional vector space based on its surrounding context of words. Each vector in this space may capture abstract representations of meaning in a specific language. Doc2Vec extends this representation to documents by averaging word2vec scores across documents.The 'documents' in this case are the dating profile essays themselves. 

We harness the Gensim library in Python, build a corpus of our 18000 texts and then build a 300-dimensional vector space based on standard recommendations for the English language. Through experimentation with different model sizes (optimizing for efficiency of code on the linguistic composition of our specific data), we choose the top 50 most influential vectors, i.e.- whose abstract dimensions may be combined to convey the majority of 'meaning'in our data. 

### Topic Modeling

A topic is defined as a mixture of words where each word has a probability of generating from a topic. The example would be words such as 'books', 'college', 'MOOC' could come from the topic of study. A document is a mixture of topics, where a single document can have covered multiple topics. The example would be that a user talks about topics related to career, study, hobbies, and religion in the self-introduction.


### DBSCAN

As stated earlierthe Doc2Vec model failed to produce any meaningful clusters

As we wanted to determine factors behind why a profile might stick out, we decided to use 
a DBSCAN model was used to detect outliers within a data-set of 50 vectors calculated by Doc2Vec. As Doc2Vec vectors capture different characteristics about the text, any outliers in these vectors should be profiles that deviate from the norm to some degree. As shown below, these Doc2Vec vectors are highly independent with very few correlations, but also highly clusterable, with a Hopkins Statistic of 0.808.

![Doc2Vec Correlation Plot](../Clustering/dbscan_doc2vec_files/figure-gfm/data-1.png)

Using a K-nearest-neighbors distribution plot, we determined that the optimal value for the epsilon neighborhood size of the DBSCAN model was 9. This was determined using 5 nearest neighbors, to match the default minimum number of points in the epsilon region, which we used for the model.

![DBSCAN KNN Distribution Plot](../Clustering/dbscan_doc2vec_files/figure-gfm/dbscan-1.png)

The DBSCAN model identified one cluster, and 108 outliers, accounting for 0.57% of the observations. Using a Wilcox Test, we can determine that the difference in the means between the "typical" profiles and outlier profiles are insignificant for the continuous variables for height and number of long words, but are highly significant for Flesch score and age. Mean age is higher among the outliers by about 4.5 years, while the mean Flesch score of outliers is about 8 points higher. This suggests that the outliers, being older and writing at a more advanced level, might be more educated (or at least trying to appear so) than their "typical" peers.

![DBSCAN Cluster Plot](../Clustering/dbscan_doc2vec_files/figure-gfm/dbscan-2.png)



## Structural Topic Modeling

### Motivation

So far, we have explored the patterns within the demographic variables using clustering and self-introduction text using LDA. However, in order to quantify whether people of different backgrounds write different topics, we need to model the topic distributions as a function of the metadata (demographic variables and DBScan score). The structural topic modeling (STM) solves the problem as it allows us to estimate the relationships between topic proportions and document metadata. Similar to the Latent Dirichlet Allocation model, STM also assumes there are some latent document-topic and topic-word distributions generating documents. However, STM differs from LDA as it handles the document-associated metadata. As a result, STM allows us to predict how topical prevalence (proportion of a topic across multiple documents) or topic content (the topic composition of terms) would shift when the metadata changes. 

### Estimation

We use the `stm` R package to estimate the model, summarize results, and visualize with the word cloud and topic network. In our project, we are interested in the topical prevalence. So the response (dependent variable) is the proportion of a topic across multiple self-introductions. The metadata (independent variables) is 'fit', 'education', 'height_group', 'race_ethnicity', and 'dbscan_cluster'. 

In the preparation step, we use `textProcessor` function to stem the words and remove stopwords. Then we use `prepDocuments` function to structure and index the data. As the low-frequency words are probably the more memorable ones, we set the 'lower.thresh' option to 0 to keep all words. 

Then we estimate the model for the topic prevalence using 9 topic models. During the estimation, the proportions of 9 topics are regressed on the metadata. There are many ways to visualize the results. The following plot shows the expected topic proportions 

![Top Topic](Plots/toptopic.jpeg)

The proportions across 9 topics do not have a large gap, as the most common topic is 0.14 and the least common one is 0.10. We observe that the top three topics seem to consist of common nouns and verbs that are not very memorable. So we further explore the most frequent words in the model for both the most common topics (#5, #4, and #7) and the least common topics (#1, #2, and #9). 

![Most Common Topics](Plots/top3topic.jpeg) ![Least Common Topics](Plots/bottom3topic.jpeg)

Although it's hard to tell the quantified difference, we find the words in the least common topics are a little more memorable, such as 'intelligent' and 'dance'. 

When we plot the histograms of topics, we find that most of the topics consist of around 10% of the topics of the documents. Interestingly, the histograms of topics #6 and #8 seem to have a bell-curve shape, which suggests there are many documents have higher or lower proportions of 10%. 
![Most Common Topics](Plots/hist.jpeg) 

In particular, we plot these two plots on the same line. Topic #6 is about 'friend' and 'live' and topic #8 is about 'year' and 'get'. So our interpretation is that topic #6 s about casually looking for friends while topic #8 is about setting a goal of getting something new in the year. As a result, although most users talk about them explicitly, some talk them more and some use other expressions.
![Comparison of Topics 6 and 8](Plots/comparison.jpeg) 

Also, `topicCorr()` calculates correlations between topics, where positive correlations mean that both topics are likely to be discussed within a document.

![Topic Networks](Plots/network.jpeg) 

For the OkCupid beginners, it will be helpful if they could learn what topics are they expected to write together. However, this would only make an ordinary self-introduction. To make it really memorable and stand out, they should consider writing a few topics that are unique and quirky.


### Evaluation

To further understand whether the topics make sense, we use the topicQuality() function to plot the semantic coherence and exclusivity values associated with each topic.

![Topic Quality](Plots/topicQuality.jpeg) 

The results seem to highly correlated with the topic proportions, as topics with higher proportions also have higher semantic coherence and exclusivity values.

### Effect

Finally, we use `estimateEffect()` function to estimating relationships between metadata and topic prevalence. There are 9 regressions under the hood and 12 coefficients associated with each. To effectively analyze the results with 108 coefficients, we compile them into a table by indicating the coefficients which are statistically significant at the level of 5%. These significant coefficients suggest that we can reject the null hypothesis and accept the alternative hypothesis that a relationship exists between the topic prevalence and metadata.

![Significant Coefficients](Plots/Covariates.png) 

We observe from the table that the factor "edu_More than HIgh Schoo" is significant at all of the 9 regressions. This means that being more educated is associated with some variations in the topic proportions. Intuitively, this suggests more educated people might want to write certain topics that make them memorized as intelligent and knowledgable. 

We notice 'height_groupshort' is only significant for regression #7. Topic # 7 has high probability words such as "like, also, think, someone", which seems quite ordinary. It's likely the use of 'also' indicates they are disconfident when introducing themselves, which might because they feel they are too short.

When looking at the coefficients at each regression, we are surprised that in the regression #6, only edu_More than HIgh Schoo" matters for Topic #6's proportions. In comparison, 8 out of the 11 coefficients are significant in the regression #8. Our current understanding is that for topic#8, people with different backgrounds have various ideas of how much to write about it. 

In summary, we get some interesting results during our first time using structural topic modeling. Although we are still learning it, we believe it has great potential to undercover the underlying relationships between metadata and topic prevalence. Such relationships would be useful for OkCupid to provide users with writing tips to make their self-introduction more memorable.


## Output

<img width="30%" src="preference_selection.png"/>
<img width="30%" src="profile_help.png"/>
<img width="30%" src="profile_help_fixed.png"/>

The above three pictures show the protocol of the final product that we have in mind. The protocol uses the example of Li (one of the project members), although the information is hypothetical. The first picture displays his demographic information and a short self-introduction. The second picture shows that the new algorithm highlights the common and memorable words by comparing them with the frequencies among his peers. He also gets a score of 63 and some tips on improving the writing. The third picture is the revised version of previous writing, which now Li has a memorable self-introduction with an increased score of 87. He should be confident to reach out to other users on OkCupid now. 


## Discussion

The project faces several limitations at this stage. Firstly, without data on the outcome (such as the number of messages), we do not know have a relative measure for the self-introductions' memorabilities. As a result, we don't have the outcomes to train the scoring function with machine learning. Secondly, without follow-up interviews, we cannot measure whether the specific choice of words was aimed at authenticity or matches with an awareness of 'relationshopping' (experienced users use certain phrases fraudulently to hook others). Thirdly, without the users' other profile images, we cannot estimate the effect of a memorable self-introduction on outcomes as the quality of images is a potential moderating variable.

# Conclusion

In this project, we apply different unsupervised machine learning methods learned from class, especially clustering and text mining. Although we have got some interesting results, we realize that there are many other alternative methods that are potentially helpful, such as factor model and association rule mining. Moreover, we feel excited to learn how other teams use UML to address the research problems creatively during the working group and final presentation. This experience greatly increases our interest to study more UML methods in the future.



\newpage
# Appendices

## Data Cleaning Code


## Kmeans Code

```{r code = readLines(knitr::purl(here("Clustering", "Kmeans.Rmd"), documentation = 1)), echo = T, eval = F}

```

## AGNES Code

```{r code = readLines(knitr::purl(here("Clustering", "advanced_clustering.Rmd"), documentation = 1)), echo = T, eval = F}

```

## DBSCAN Code

```{r code = readLines(knitr::purl(here("Clustering", "dbscan_doc2vec.Rmd"), documentation = 1)), echo = T, eval = F}

```

## Doc2Vec Code

```{python code = readLines(flatten_jupyter(here("Vector_Space_Model", "Doc2Vec_Modelling.ipynb"))), echo = T, eval = F}

```

## Structural topic modeling Code

```{r code = readLines(knitr::purl(here("Structural Topic Models", "stm-demo+dbscan.Rmd"), documentation = 1)), echo = T, eval = F}

```

\newpage
# References
