{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from string import punctuation\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from spacy.en import English\n",
    "\n",
    "from utils.permutation import print_pvalues\n",
    "from utils.text_representation import _levels, _multinomial\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = English(tagger=True, entity=False)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Functions\n",
    "# Text Representation\n",
    "\n",
    "\n",
    "def _levels(demographics, d_levels=None, print_levels=False):\n",
    "    \"\"\"The demographic levels to iterate over\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    demographics : pd.Series\n",
    "        Demographic labels\n",
    "    d_levels : list, default None\n",
    "        The specific demographic levels desired\n",
    "    print_levels : bool, default False\n",
    "        Whether to print the demographic levels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    levels : iterable\n",
    "        The unique (sorted) levels in `demographics`\n",
    "    \"\"\"\n",
    "    levels = demographics.unique()\n",
    "    if d_levels:\n",
    "        assert set(d_levels).issubset(levels)\n",
    "        levels = d_levels\n",
    "    levels.sort()\n",
    "    if print_levels:\n",
    "        print('Levels (in order):', levels, end='\\n\\n')\n",
    "    return levels\n",
    "\n",
    "def _multinomial(corpus, kwargs):\n",
    "    \"\"\"Tokens counts by document using the spaCy tokenizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : array-like\n",
    "        A collection of documents\n",
    "    kwargs : dict or None\n",
    "        Keyword arguments of variable length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : scipy.sparse.csr.csr_matrix\n",
    "        The multinomial representation shape (n_samples, n_features)\n",
    "    v : list\n",
    "        Vocabulary\n",
    "    \"\"\"\n",
    "    if kwargs:\n",
    "        cv = CountVectorizer(tokenizer=spacy_tokenize, **kwargs)\n",
    "    else:\n",
    "        cv = CountVectorizer(tokenizer=spacy_tokenize)\n",
    "    X = cv.fit_transform(corpus)\n",
    "    v = cv.get_feature_names()\n",
    "    return X, v\n",
    "\n",
    "def print_pvalues(a, b):\n",
    "    \"\"\"Wrapper function for printing meand and p-values\n",
    "    both permutation-based and classical\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : np.ndarray\n",
    "        Data for one class or\n",
    "        ground truth (correct) labels\n",
    "    b : np.ndarray\n",
    "        Data for another class or\n",
    "        predicted labels, as returned by a classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    assert isinstance(a, np.ndarray) and isinstance(b, np.ndarray)\n",
    "    rnd = lambda x: np.round(x, 8)\n",
    "    permutation = _permute(a, b, 'means')\n",
    "    classical = ttest_ind(a, b, equal_var=False)[1]\n",
    "    print(\"[means] 'a':\", rnd(a.mean()), \"'b':\", rnd(b.mean()))\n",
    "    print(\"p-values:\")\n",
    "    print(\"  [permutation]:\", rnd(permutation))\n",
    "    print(\"  [classical]:  \", rnd(classical))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "def nmf_labels(tfidfmatrix, k):\n",
    "    \"\"\"For getting the labels (group assignment) associated with\n",
    "    each sample (user, in this case)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tfidfmatrix : scipy.sparse.csr.csr_matrix\n",
    "        The output from calling `TfidfVectorizer` on the users/features data\n",
    "\n",
    "    k : int\n",
    "        The number of groupings to create\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : np.ndarray\n",
    "        An array of group assignments of length tfidfmatrix.shape[0] (users)\n",
    "    \"\"\"\n",
    "    H = NMF(n_components=k, random_state=42).fit_transform(tfidfmatrix)\n",
    "    labels = np.argmax(H, axis=1)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def nmf_inspect(tfidfmatrix, feature_names, k_vals=[3, 5, 7, 9], n_words=10):\n",
    "    \"\"\"For looping over various values of `k` and printing the\n",
    "    top `n_words`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tfidfmatrix : scipy.sparse.csr.csr_matrix\n",
    "        The output from calling `TfidfVectorizer` on the users/features data\n",
    "\n",
    "    feature_names : list\n",
    "        The output from calling the `.get_feature_names()` on\n",
    "        the TfidfVectorizer object\n",
    "\n",
    "    k_vals : list\n",
    "        A list of values for `k`, the number of groupings\n",
    "\n",
    "    n_words : int\n",
    "        The top n words to print for each grouping\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for k in k_vals:\n",
    "        nmf = NMF(n_components=k, random_state=42).fit(tfidfmatrix)\n",
    "        print(k, end='\\n')\n",
    "        _print_words(nmf, feature_names, n_words)\n",
    "        \n",
    "def subset_df(df, col, vals):\n",
    "    \"\"\"Return a subset of `df` based on particular `vals` for `col`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    col : str\n",
    "        Valid column name\n",
    "    vals : list\n",
    "        Values to subset on\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subset : pd.DataFrame\n",
    "        The rows in `df` with values in `val` for `col`\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    subset = df[df[col].isin(vals)]\n",
    "    return subset\n",
    "\n",
    "def group_pct(df, demographic):\n",
    "    \"\"\"Calculate the percentage of users in each `demographic` level\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Where applicable, this should be a subset of the original DataFrame and \n",
    "        should include a `group` column corresponding to the NMF groupings\n",
    "    demographic : str\n",
    "        Valid column name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    by_dg : pd.DataFrame\n",
    "        Including `demographic` levels and `group` percentages\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    by_dg = pd.DataFrame({'count' :\n",
    "                          df.groupby([demographic, 'group'])['group'].count()}).reset_index()\n",
    "    by_d = by_dg.groupby(demographic, as_index=False)['count'].sum()\n",
    "    by_dg = pd.merge(by_dg, by_d, on=demographic)\n",
    "    by_dg['pct'] = by_dg.count_x / by_dg.count_y\n",
    "    return by_dg\n",
    "\n",
    "def feature_vectors(corpus, kwargs=None):\n",
    "    \"\"\"Multinomial and TF-IDF representations\n",
    "\n",
    "    Paramaters\n",
    "    ----------\n",
    "    corpus : array-like\n",
    "        A collection of documents\n",
    "    kwargs : dict, default None\n",
    "        Keyword arguments of variable length\n",
    "        See sklearn.feature_extraction.text.CountVectorizer\n",
    "        for accepted keyword arguments\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    count : scipy.sparse.csr.csr_matrix\n",
    "        The multinomial representation shape (n_samples, n_features)\n",
    "    tfidf : scipy.sparse.csr.csr_matrix\n",
    "        The tf-idf representation\n",
    "    vocab : list\n",
    "        Vocabulary\n",
    "    \"\"\"\n",
    "    assert isinstance(corpus, (list, pd.Series))\n",
    "    count, vocab = _multinomial(corpus, kwargs)\n",
    "    tfidf = _tfidf(count)\n",
    "    return count, tfidf, vocab\n",
    "\n",
    "\n",
    "def tagger(doc):\n",
    "    \"\"\"For tagging a document\n",
    "    Yields a (token, part-of-speech) tag tuple\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "        A document with tokens to tag\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        (token, tag)\n",
    "    \"\"\"\n",
    "    text = nlp(doc)\n",
    "    for sent in text.sents:\n",
    "        for token in sent:\n",
    "            yield (str(token), str(token.pos_))\n",
    "\n",
    "def tag_corpus(corpus):\n",
    "    \"\"\"For tagging corpus document tokens\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : array-like\n",
    "        A collection of documents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tagged : list\n",
    "        (token, tag) tuples\n",
    "    \"\"\"\n",
    "    assert isinstance(corpus, (list, pd.Series))\n",
    "    tagged = []\n",
    "    for doc in corpus:\n",
    "        tagged.extend(tagger(doc))\n",
    "    return tagged\n",
    "\n",
    "def pos_tokens(tagged, pos):\n",
    "    \"\"\"Extract particular part-of-speech tokens\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tagged : list\n",
    "        (token, tag) tuples\n",
    "    pos : str\n",
    "        A valid part-of-speech tag\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The available tags are:\n",
    "        ADJ, ADP, ADV, AUX, CONJ, DET, INTJ, NOUN, NUM, PART,\n",
    "        PRON, PROPN, PUNCT, SCONJ, SYM, VERB, X, EOL, SPACE\n",
    "    Source: https://spacy.io/docs#token-postags\n",
    "    \"\"\"\n",
    "    return [t for t, p in tagged if p == pos]\n",
    "\n",
    "def _pos_freq(doc):\n",
    "    \"\"\"Part of speech frequencies for individual documents\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    doc : str\n",
    "        A document with tokens to tag\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pos : dict\n",
    "        With counts by tag\n",
    "    \"\"\"\n",
    "    pos = defaultdict(float)\n",
    "    for _, p in tagger(doc):\n",
    "        pos[p] += 1\n",
    "    return pos\n",
    "\n",
    "def pos_df(corpus):\n",
    "    \"\"\"Create a DataFrame of part of speech\n",
    "    frequencies for a corpus of documents\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : array-like\n",
    "        A collection of documents\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "    \"\"\"\n",
    "    assert isinstance(corpus, (list, pd.Series))\n",
    "    pos_dfs = []\n",
    "    for doc in corpus:\n",
    "        frequencies = pd.DataFrame(_pos_freq(doc), index=[0])\n",
    "        pos_dfs.append(frequencies)\n",
    "    df = pd.concat(pos_dfs, ignore_index=True)\n",
    "    df.fillna(0.0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def pos_normalize(df):\n",
    "    \"\"\"Normalize (row-wise) part-of-speech frequencies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        `pos_df()` DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    return (df.T / df.sum(axis=1)).T\n",
    "\n",
    "def _arrs_pos(df_orig, df_pos, demographic, pos,\n",
    "              d_levels=None, print_levels=False):\n",
    "    \"\"\"Individual part-of-speech\n",
    "    arrays for a particular demographic\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_orig : pd.DataFrame\n",
    "        The DataFrame from which `df_pos` was created\n",
    "    df_pos : pd.DataFrame\n",
    "        The part-of-speech DataFrame\n",
    "    demographic : str\n",
    "        A valid demographic-data column in `df_orig`\n",
    "    pos : str\n",
    "        A column in `df_pos` corresponding\n",
    "        to a part of speech\n",
    "    d_levels : list, default None\n",
    "        The specific demographic levels desired\n",
    "    print_levels : bool, default False\n",
    "        Whether to print the demographic levels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    arrs : tuple of np.arrays\n",
    "        The corresponding `pos` values for each `demographic`\n",
    "    \"\"\"\n",
    "    df_pos = df_pos.copy() # so we don't modify it\n",
    "    df_pos[demographic] = df_orig[demographic].values\n",
    "    levels = _levels(df_orig[demographic], d_levels, print_levels)\n",
    "    arrs = []\n",
    "    for d in levels:\n",
    "        arr = df_pos[df_pos[demographic] == d][pos].values\n",
    "        n = arr.shape[0]\n",
    "        if n < 0.1 * df_pos.shape[0]:\n",
    "            print(\"Warning: '\" + d +\n",
    "                  \"' category has less than 10% of observations (\" +\n",
    "                  str(n) + \")\")\n",
    "        arrs.append(arr)\n",
    "    return tuple(arrs)\n",
    "\n",
    "def pos_by_split(df_orig, df_pos, demographic, pos=None,\n",
    "                 d_levels=None, print_levels=False):\n",
    "    \"\"\"Wrapper for handling multiple parts-of-speech with `_arrs_pos()`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_orig : pd.DataFrame\n",
    "        The DataFrame from which `df_pos` was created\n",
    "    df_pos : pd.DataFrame\n",
    "        The part-of-speech DataFrame\n",
    "    demographic : str\n",
    "        A valid demographic-data column in `df_orig`\n",
    "    pos : list, default None\n",
    "        Parts-of-speech to compare\n",
    "    d_levels : list, default None\n",
    "        The specific demographic levels desired\n",
    "    print_levels : bool, default False\n",
    "        Whether to print the demographic levels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The number of unique values in `demographic` must be two\n",
    "    \"\"\"\n",
    "    assert (isinstance(df_orig, pd.DataFrame) and\n",
    "            isinstance(df_pos, pd.DataFrame))\n",
    "    assert df_orig.shape[0] == df_pos.shape[0]\n",
    "    assert demographic in df_orig.columns\n",
    "    assert set(pos).issubset(df_pos.columns)\n",
    "    for p in pos:\n",
    "        a, b = _arrs_pos(df_orig, df_pos, demographic, p, d_levels, print_levels)\n",
    "        print(p)\n",
    "        print_pvalues(a, b)\n",
    "        print()\n",
    "\n",
    "def load_words(path):\n",
    "    \"\"\"To load profane and slang words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Relative or absolute filepath\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "    \"\"\"\n",
    "    assert isinstance(path, str)\n",
    "    with open(path, 'r') as f:\n",
    "        return list(set([w.rstrip() for w in f.readlines()]))\n",
    "\n",
    "def _contains_n(words, corpus):\n",
    "    \"\"\"Count the number of times a document contains particular words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list\n",
    "        Words to check for\n",
    "    corpus : array-like\n",
    "        A collection of documents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Number of tokens by document\n",
    "    \"\"\"\n",
    "    X, _ = _multinomial(corpus, {'vocabulary' : words})\n",
    "    return X.toarray().sum(axis=1)\n",
    "\n",
    "def contains(words, corpus):\n",
    "    \"\"\"Determine whether a document contains particular words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list\n",
    "        Words to check for\n",
    "    corpus : array-like\n",
    "        A collection of documents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_words : np.ndarray\n",
    "        Binary representation\n",
    "    \"\"\"\n",
    "    assert isinstance(words, list)\n",
    "    assert isinstance(corpus, (list, pd.Series))\n",
    "    n_words = _contains_n(words, corpus)\n",
    "    n_words[n_words > 0] = 1\n",
    "    return n_words\n",
    "\n",
    "def _token_counts(a, b, pos):\n",
    "    \"\"\"Create a DataFrame of `pos` token frequencies for particular\n",
    "    demographic splits. `a` and `b` are lists of token, part-of-speech\n",
    "    tuples (output from `tag_corpus()`).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : list\n",
    "        token, pos tuples\n",
    "    b : list\n",
    "        token, pos tuples\n",
    "    pos : str\n",
    "        A valid part-of-speech tag\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        With row 0 corresponding to `a` and row 1 to `b`\n",
    "    \"\"\"\n",
    "    pos_a = nltk.FreqDist(pos_tokens(a, pos))\n",
    "    pos_b = nltk.FreqDist(pos_tokens(b, pos))\n",
    "    df_a = pd.DataFrame(pos_a, index=[0])\n",
    "    df_b = pd.DataFrame(pos_b, index=[0])\n",
    "    df = pd.concat([df_a, df_b], ignore_index=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def print_terms(df, n):\n",
    "    measure = df.columns[0]\n",
    "    print(\" | \".join(df.sort_values(measure, ascending=False)[:n].index))\n",
    "    print()\n",
    "    print(\" | \".join(df.sort_values(measure)[:n].index))\n",
    "\n",
    "def top_terms(a, b, pos, fn, n):\n",
    "    \"\"\"Print the top `n` tokens (resulting from `fn`) for\n",
    "    demographic splits associated with `a` and `b`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : list\n",
    "        token, pos tuples\n",
    "    b : list\n",
    "        token, pos tuples\n",
    "    pos : str\n",
    "        A valid part-of-speech tag\n",
    "    fn : callable\n",
    "        Either `diff_prop` of `log_odds_ratio`\n",
    "    n : int\n",
    "        Number of terms to print for each demographic split\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    df = _token_counts(a, b, pos)\n",
    "    df = fn(df.values, df.columns.tolist())\n",
    "    print_terms(df, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we generate the topics and assign some meaning to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('compressed_okcupid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The major part of the algorithm- can take some time\n",
    "specs = {'stop_words' : 'english', 'ngram_range' : (1, 3), 'min_df' : 0.005}\n",
    "counts, tfidf, vocab = feature_vectors(df.essay0, specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 25\n",
    "nmf_inspect(tfidf, vocab, k_vals=[K], n_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These labels are based on the categories as assessed by Juan Shishido, then modified by me\n",
    "labels=['Reach Out!','Relocated', 'About Me', 'Hesitation', 'Casual', 'The City',\n",
    "       'Novelty', 'Cool', 'Likes', 'Passions', 'Easy Going', 'Region', 'Seeking', 'Thoughts', 'Fun', 'New Here',\n",
    "        'Travel','Self-summary', 'Nots', 'Growing Up','Carpe Diem', 'Good Company','Hobbies',\n",
    "        'Cultural Interests', 'Ambitious']\n",
    "\n",
    "label_dict = {}\n",
    "for c, value in enumerate(labels):\n",
    "    label_dict[c] = value\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we find a way of calculating and visualizing these topic distributions across our 4 chosen demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_demog(model, feature_names, n_top_words):\n",
    "    \"\"\"For printing the `n_top_words` for each grouping\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn.decomposition.nmf.NMF\n",
    "        The NMF object\n",
    "\n",
    "    feature_names : list\n",
    "        The output from calling `TfidfVectorizer` on the users/features data\n",
    "\n",
    "    n_top_words : int\n",
    "        The top n words to print for a particular grouping\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Group %d:\" % topic_idx)\n",
    "        print(\" | \".join([feature_names[i]\n",
    "            for i in topic.argsort()[ : -n_top_words-1 : -1]]))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(group_num):\n",
    "    return label_dict[group_num]\n",
    "\n",
    "def format_df(df, demog, tfidf): \n",
    "    df['group'] = nmf_labels(tfidf, k=K)\n",
    "    subset = subset_df(df, demog, df[demog].unique())\n",
    "    grouped = group_pct(subset, demog)\n",
    "    percent_only = grouped.drop(['count_x', 'count_y'], axis=1)\n",
    "    #percent_only\n",
    "    pivoted = percent_only.pivot(index='group', columns=demog)\n",
    "    pivoted['max_value'] = pivoted.max(axis=1)\n",
    "    ordered_df = pivoted.sort_values(by='max_value', ascending=True)\n",
    "    #Getting rid of the multi-line index\n",
    "    ordered_df.columns = ordered_df.columns.droplevel(0)\n",
    "    ordered_df = ordered_df.reset_index().rename_axis(None, axis=1)\n",
    "    #Renaming the max\n",
    "    ordered_df = ordered_df.rename(columns={'':'max'})\n",
    "    #Linking to label\n",
    "    ordered_df['label'] = ordered_df['group'].apply(get_label)\n",
    "    return ordered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_df, race_df, edu_df, fit_df= format_df(df, 'height_group', tfidf), \n",
    "                                    format_df(df, 'race_ethnicity', tfidf), \n",
    "                                    format_df(df, 'edu', tfidf), \n",
    "                                    format_df(df, 'fit', tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Education Levels\n",
    "ordered_df = edu_df\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "my_range=range(1,len(ordered_df.index)+1)\n",
    "fig, ax = plt.subplots(figsize=(18, 15))\n",
    "ttl = ax.title\n",
    "ttl.set_position([.5, 1.05])\n",
    "\n",
    "# The vertival plot is made using the hline function\n",
    "# I load the seaborn library only to benefit the nice looking feature\n",
    "import seaborn as sns\n",
    "plt.hlines(y=my_range, xmin=0, xmax=ordered_df['max'], color='Gray')\n",
    "plt.plot(ordered_df['High School or less'], my_range, \"o\", markersize=20, color='blue')\n",
    "plt.plot(ordered_df['More than High School'], my_range, \"o\", markersize=20, color='red')\n",
    "plt.rc('ytick',labelsize=28)\n",
    "plt.rc('xtick',labelsize=28)\n",
    "# Add titles and axis names\n",
    "plt.yticks(my_range, ordered_df['label'])\n",
    "plt.title(\"Topics in OkCupid Male Self-Introductions Across Education Levels\", loc='center', fontsize=40)\n",
    "plt.xlabel('Proportion of Users Using This Topic', fontsize=32)\n",
    "plt.ylabel('Topics Inferred from Essay',fontsize=32)\n",
    "maroon_patch = mpatches.Patch(color='red', label='More than High School')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Less than High School')\n",
    "plt.legend(handles=[maroon_patch, blue_patch], loc='center right', fontsize='xx-large', borderpad=2)\n",
    "plt.savefig('opinions.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Fitness Levels\n",
    "ordered_df = fit_df\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "my_range=range(1,len(fit_df.index)+1)\n",
    "fig, ax = plt.subplots(figsize=(18, 15))\n",
    "ttl = ax.title\n",
    "ttl.set_position([.5, 1.05])\n",
    "\n",
    "# The vertival plot is made using the hline function\n",
    "# I load the seaborn library only to benefit the nice looking feature\n",
    "import seaborn as sns\n",
    "plt.hlines(y=my_range, xmin=0, xmax=ordered_df['max'], color='Gray')\n",
    "plt.plot(ordered_df['fit'], my_range, \"o\", markersize=20, color='blue')\n",
    "plt.plot(ordered_df['not_fit'], my_range, \"o\", markersize=20, color='red')\n",
    "plt.rc('ytick',labelsize=28)\n",
    "plt.rc('xtick',labelsize=28)\n",
    "# Add titles and axis names\n",
    "plt.yticks(my_range, ordered_df['label'])\n",
    "plt.title(\"Topics in OkCupid Male Self-Introductions Across Fitness Levels\", loc='center', fontsize=40)\n",
    "plt.xlabel('Proportion of Users Using This Topic', fontsize=32)\n",
    "plt.ylabel('Topics Inferred from Essay',fontsize=32)\n",
    "maroon_patch = mpatches.Patch(color='red', label='Fit')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Not Fit')\n",
    "plt.legend(handles=[maroon_patch, blue_patch], loc='center right', fontsize='xx-large', borderpad=2)\n",
    "plt.savefig('fit.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Plot for Height\n",
    "ordered_df = height_df\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "my_range=range(1,len(ordered_df.index)+1)\n",
    "fig, ax = plt.subplots(figsize=(18, 15))\n",
    "ttl = ax.title\n",
    "ttl.set_position([.5, 1.05])\n",
    "\n",
    "# The vertival plot is made using the hline function\n",
    "# I load the seaborn library only to benefit the nice looking feature\n",
    "import seaborn as sns\n",
    "plt.hlines(y=my_range, xmin=0, xmax=ordered_df['max'], color='Gray')\n",
    "plt.plot(ordered_df['short'], my_range, \"o\", markersize=20, color='blue')\n",
    "plt.plot(ordered_df['not_short'], my_range, \"o\", markersize=20, color='red')\n",
    "plt.rc('ytick',labelsize=28)\n",
    "plt.rc('xtick',labelsize=28)\n",
    "# Add titles and axis names\n",
    "plt.yticks(my_range, ordered_df['label'])\n",
    "plt.title(\"Topics in OkCupid Male Self-Introductions Across Height Groups\", loc='center', fontsize=40)\n",
    "plt.xlabel('Proportion of Users Using This Topic', fontsize=32)\n",
    "plt.ylabel('Topics Inferred from Essay',fontsize=32)\n",
    "maroon_patch = mpatches.Patch(color='red', label='Short')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Not Short')\n",
    "plt.legend(handles=[maroon_patch, blue_patch], loc='center right', fontsize='xx-large', borderpad=2)\n",
    "plt.savefig('height.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Plot for Races\n",
    "ordered_df = race_df\n",
    "my_range=range(1,len(ordered_df.index)+1)\n",
    "fig, ax = plt.subplots(figsize=(18, 15))\n",
    "ttl = ax.title\n",
    "ttl.set_position([.5, 1.05])\n",
    "\n",
    "# The vertival plot is made using the hline function\n",
    "# I load the seaborn library only to benefit the nice looking feature\n",
    "import seaborn as sns\n",
    "plt.hlines(y=my_range, xmin=0, xmax=ordered_df['max'], color='Gray')\n",
    "plt.plot(ordered_df['White'], my_range, \"o\", markersize=20, color='blue')\n",
    "plt.plot(ordered_df['Black'], my_range, \"o\", markersize=20, color='red')\n",
    "plt.plot(ordered_df['Asian'], my_range, \"o\", markersize=20, color='green')\n",
    "plt.plot(ordered_df['Latinx'], my_range, \"o\", markersize=20, color='cyan')\n",
    "plt.plot(ordered_df['multiple'], my_range, \"o\", markersize=20, color='magenta')\n",
    "plt.rc('ytick',labelsize=28)\n",
    "plt.rc('xtick',labelsize=28)\n",
    "# Add titles and axis names\n",
    "plt.yticks(my_range, ordered_df['label'])\n",
    "plt.title(\"Topics in OkCupid Male Self-Introductions Across Racial Groups\", loc='center', fontsize=40)\n",
    "plt.xlabel('Proportion of Users Using This Topic', fontsize=32)\n",
    "plt.ylabel('Topics Inferred from Essay',fontsize=32)\n",
    "blue_patch = mpatches.Patch(color='blue', label='White')\n",
    "maroon_patch = mpatches.Patch(color='red', label='Black')\n",
    "green_patch = mpatches.Patch(color='green', label='Asian')\n",
    "cyan_patch = mpatches.Patch(color='cyan', label='Latinx')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='multiple')\n",
    "plt.legend(handles=[maroon_patch, blue_patch, green_patch, cyan_patch, magenta_patch], loc='center right', fontsize='xx-large', borderpad=2)\n",
    "plt.savefig('race.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
