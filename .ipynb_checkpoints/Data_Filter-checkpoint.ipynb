{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filter\n",
    "### The purpose of this notebook is four-fold:\n",
    "1) Filter data to only the relevant rows\n",
    "\n",
    "2) Delete the unnecessary columns\n",
    "\n",
    "3) Suitably edit the text to allow for topic modeling\n",
    "\n",
    "4) Create new variables to assist with demographic comparisons of topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the CRISP-DM Framework for Data Analysis here (as outlined here:\n",
    "https://www.datascience-pm.com/crisp-dm-2/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Business understanding – What does the business need?*\n",
    "The business, in this case, is OkCupid, and it might be facing a large number of men dropping out of the system because of high competition and limited responses. We cannot confirm this directly with OkCupid, but we do know that self-representation through online dating is a relatively new skill to gain in our species' long and checkered history. So why not provide some guidance along the way? The goal would be to provide data-driven guidance to male users of the service so that they stand out from the competition and get matched more often. This will increase the rating of the app, and lead to more sign-ups and revenue. \n",
    "\n",
    "Getting present-day data would be challenging. Many researchers have gained access to profiles and conversation data, but usually have the funding support and credentials of their universities to back them. Moreover, online dating data involves considerable privacy concerns. In such a situation, it would be best to acquire a low-cost data set that anonymizes data, but need not include all aspects of profiles or even be up-to-date. \n",
    "\n",
    "Success for us would involve first testing for the extent of homogeneity in dating profiles, and then providing support with helpful UX features that provide tips to remove that homeogeneity and sound memorable vis-a-vis other users. In technical terms, this means identifying from text the most common topics, language patterns and keywords, and then providing guidance to prevent such repetition. It would also be useful to check if these patterns vary in different subgroups of users, as indicated by variables like height, weight/fitness level, race and education level. \n",
    "\n",
    "Given these objectives, we will be proceeding with using Python and R (depending on which of them contains the most suitable packages for our specific and evolving tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data understanding – What data do we have / need? Is it clean?**\n",
    "\n",
    "\n",
    "\n",
    "Collect initial data: Acquire the necessary data and (if necessary) load it into your analysis tool.\n",
    "Describe data: Examine the data and document its surface properties like data format, number of records, or field identities.\n",
    "Explore data: Dig deeper into the data. Query it, visualize it, and identify relationships among the data.\n",
    "Verify data quality: How clean/dirty is the data? Document any quality issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation – How do we organize the data for modeling?**\n",
    "\n",
    "Fortunately, this is our sole dataset and does not seem to require any form of integration\n",
    "\n",
    "    Select data: Determine which data sets will be used and document reasons for inclusion/exclusion.\n",
    "    Clean data: Often this is the lengthiest task. Without it, you’ll likely fall victim to garbage-in, garbage-out. A common practice during this task is to correct, impute, or remove erroneous values.\n",
    "    Construct data: Derive new attributes that will be helpful. For example, derive someone’s body mass index from height and weight fields.\n",
    "    Integrate data: Create new data sets by combining data from multiple sources.\n",
    "    Format data: Re-format data as necessary. For example, you might convert string values that store numbers to numeric values so that you can perform mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling – What modeling techniques should we apply?**\n",
    "\n",
    "We are focused on different versions of topic models here. One choice is between the commonly used Latent Dirichlet Allocation (LDA). This model is advantageous because it can be built on for more complex models, such as Structural Topic Models. \n",
    "\n",
    "On the other hand, the alternative Non-Negative Matrix Factorization model may be useful too, and has been shown to offer many advantages over LDA when dealing with very short documents such as SMS and Tweets (see references in Thesis- linked in the readme). \n",
    "\n",
    "We will need to check how these models play out on our data, and choose accordingly. However, a priori, it seems like \n",
    "\n",
    "    Select modeling techniques: Determine which algorithms to try (e.g. regression, neural net).\n",
    "    Generate test design: Pending your modeling approach, you might need to split the data into training, test, and validation sets.\n",
    "    Build model: As glamorous as this might sound, this might just be executing a few lines of code like “reg = LinearRegression().fit(X, y)”.\n",
    "    Assess model: Generally, multiple models are competing against each other, and the data scientist needs to interpret the model results based on domain knowledge, the pre-defined success criteria, and the test design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation – Which model best meets the business objectives?\n",
    "\n",
    "    Evaluate results: Do the models meet the business success criteria? Which one(s) should we approve for the business?\n",
    "    Review process: Review the work accomplished. Was anything overlooked? Were all steps properly executed? Summarize findings and correct anything if needed.\n",
    "    Determine next steps: Based on the previous three tasks, determine whether to proceed to deployment, iterate further, or initiate new projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deployment – How do stakeholders access the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# For Data Cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "from split_utils import *\n",
    "from text_complexity_utils import get_npoly, get_flesch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in raw data\n",
    "df = pd.read_csv('../profiles.csv/profiles.csv')\n",
    "#correct subset of data\n",
    "df = df[(df['sex']==\"m\")\n",
    "        &(df['orientation']==\"straight\") \n",
    "        & (df['status']==\"single\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29163, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 29163/29163 [00:12<00:00, 2302.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Some of the essays have just a link in the text. BeautifulSoup sees that and gets \n",
    "# the wrong idea. This line hides those warnings.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Takes in raw text of essays\n",
    "    Removes all null values and url links\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    text: string\n",
    "        Usually, this is the raw profile essay \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t: string\n",
    "        This refers to the cleaned profile essay\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        t = np.nan\n",
    "    else:\n",
    "        t = BeautifulSoup(text, 'lxml').get_text()\n",
    "        t = t.lower()\n",
    "        t = t.strip().replace('\\n','').replace(\"\\r\", \" \").replace('\\t', '')\n",
    "        bad_words = ['http', 'www', '\\nnan']\n",
    "\n",
    "        for b in bad_words:\n",
    "            t = t.replace(b, '')\n",
    "    #After these subsitutions, the string may become empty\n",
    "    if t == '':\n",
    "        t = np.nan\n",
    "    \n",
    "    return t\n",
    "\n",
    "#Clearing out all HTML and unnecessary characters\n",
    "df['essay0'] = df['essay0'].progress_apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables of interest are categorical, and therefore not easily imputed. \n",
    "It may be possible to impute missing height. But the remaining categorical values in the data bear no causal relationship with height (other than perhaps, race) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus on the chosen variables of importance and the essay\n",
    "must_haves = ['body_type', 'height', 'education', 'ethnicity', 'sex', 'essay0']\n",
    "#drop the rest\n",
    "df = df[must_haves]\n",
    "#drop null values\n",
    "df = df.dropna(subset= must_haves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20576, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING NEW COLUMNS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the sections here are taken directly from the following link, with specific modifications\n",
    "Taken directly from:\n",
    "https://github.com/UM-CSS/CSSLabs-NLP/blob/master/1_Data_munging.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode(text, dictionary, default=np.nan):\n",
    "    \"\"\"\n",
    "    Function for recoding categories in a column based on exact matches\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: a string\n",
    "    \n",
    "    dictionary: dictionary\n",
    "        contains desired values as keys, and all the\n",
    "        labels to be matched with it used as values\n",
    "    \n",
    "    default: string or None\n",
    "        the value to be used if no match is found with the \n",
    "        dictionary keys\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    out: a string or None\n",
    "    \n",
    "    \"\"\"\n",
    "    out = default\n",
    "    text = str(text)\n",
    "    \n",
    "    for x in dictionary.keys():\n",
    "        for y in dictionary[x]:\n",
    "            if y == text: #exact match\n",
    "                out = x\n",
    "                return out\n",
    "    return out\n",
    "\n",
    "#Might be possible to refactor this function out completely\n",
    "def recode_fuzzy(text, dictionary, default=np.nan):\n",
    "    \"\"\"\n",
    "    Function for recoding categories in a column based on partial matches\n",
    "    \n",
    "    text: a string\n",
    "    \n",
    "    dictionary: dictionary\n",
    "        contains desired values as keys, and all the\n",
    "        labels to be matched with it used as values\n",
    "    \n",
    "    default: string or None\n",
    "        the value to be used if no match is found with the \n",
    "        dictionary keys\n",
    "        \n",
    "    Returns\n",
    "    ------\n",
    "    out: a string or None\n",
    "\n",
    "    \"\"\"\n",
    "    out = default\n",
    "    text = str(text)\n",
    "    \n",
    "    for x in dictionary.keys():\n",
    "        for y in dictionary[x]:\n",
    "            if y in text: #partial match\n",
    "                out = x\n",
    "                return out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tese dictionaries were created from all the observed unique values\n",
    "\n",
    "#Education\n",
    "ed_levels = {'High School or less': ['dropped out of high school', 'working on high school','graduated from high school', 'working on college/university', \n",
    "                    'two-year college', 'dropped out of college/university', \n",
    "                    'high school'], \n",
    "             'More than High School': ['graduated from college/university', \n",
    "                    'working on masters program', 'working on ph.d program', \n",
    "                    'college/university', 'working on law school', \n",
    "                    'dropped out of masters program', \n",
    "                    'dropped out of ph.d program', 'dropped out of law school', \n",
    "                    'dropped out of med school',\n",
    "                    'graduated from masters program',\n",
    "                    'graduated from ph.d program',                           \n",
    "                    'graduated from law school', \n",
    "                    'graduated from med school', 'masters program', \n",
    "                    'ph.d program', 'law school', 'med school']}\n",
    "\n",
    "#body type\n",
    "bodies = {'fit': ['fit', 'athletic', 'jacked'], \n",
    "          'not_fit': ['average', 'thin', 'skinny','curvey', 'a little extra', \n",
    "                      'full figured', 'overweight', 'rather not say', 'used up']\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edu'] = df.education.apply(recode, dictionary=ed_levels, \n",
    "                                            default='unknown')\n",
    "df['fit'] = df.body_type.apply(recode, dictionary=bodies, \n",
    "                                            default='unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race/ethnicity for exact matching\n",
    "ethn = {'White': ['white', 'middle eastern', 'middle eastern, white'], \n",
    "        'Asian': ['asian', 'indian', 'asian, pacific islander'], \n",
    "        'Black': ['black']\n",
    "       }   \n",
    "\n",
    "# race/ethnicityfor fuzzy matching\n",
    "ethn2 = {'Latinx': ['latin'], \n",
    "         'multiple': [','], \n",
    "         np.nan: ['nan']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def census_2010_ethnicity(t):\n",
    "    '''\n",
    "    recodes ethnicity variables according to census categories\n",
    "    This conversion happens through dictionaries declared in the\n",
    "    previous cell. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t- string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    e- string\n",
    "    '''\n",
    "    text = str(t)\n",
    "    e = recode(text, ethn, default='other')\n",
    "    if 'other' == e:\n",
    "        e = recode_fuzzy(text, ethn2, default='other')\n",
    "    return e\n",
    "\n",
    "df['race_ethnicity'] = df.ethnicity.apply(census_2010_ethnicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there may be some way to build in the calculation of the first quartile\n",
    "def height_check(inches):\n",
    "    \"\"\"\n",
    "    takes in height and returns a label of short or not short\n",
    "    uses the first quartile as the cutoff for not being short\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    inches: float\n",
    "        The height of the user in inches\n",
    "    \n",
    "    returns\n",
    "    ------\n",
    "    h: string\n",
    "        A label- 'short' or 'not short'\n",
    "    \n",
    "    \"\"\"\n",
    "    h = 'not_short'\n",
    "    if inches <= 69:\n",
    "        #This number was extracted as the first quartile of the distribution of height\n",
    "        h = 'short'\n",
    "    return h\n",
    "df['height'] = pd.to_numeric(df['height'])\n",
    "df['height_group'] = df.height.apply(height_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now drop the original variables\n",
    "df.drop(columns=['body_type', 'ethnicity','height','education'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('profiles_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROFILE LENGTH AND VARIABLES OF INTEREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Ethnicity\n",
    "sns_race_plot = sns.boxplot(x=\"race_ethnicity\", y=\"profile_length\", data=df)\n",
    "sns_race_plot.set(title = 'Racial Background and Length of Dating Profile', \n",
    "                  xlabel = 'Race', ylabel = 'Number of Words')\n",
    "sns_race_plot.figure.savefig('profile_race.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Education \n",
    "sns_plot = sns.boxplot(x=\"edu\", y=\"profile_length\", data=df)\n",
    "sns_plot.set(title = 'Education and Length of Dating Profile', \n",
    "                                                           xlabel = 'Education', \n",
    "                                                           ylabel = 'Number of Words' )\n",
    "sns_plot.figure.savefig('profile_educ.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Height \n",
    "sns_plot = sns.boxplot(x=\"height_group, y=\"profile_length\", data=df)\n",
    "sns_plot.set(title = 'Height and Length of Dating Profile', \n",
    "                                                           xlabel = 'Height', \n",
    "                                                           ylabel = 'Number of Words' )\n",
    "sns_plot.figure.savefig('profile_height.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Ditness Level\n",
    "sns_plot = sns.boxplot(x=\"fit, y=\"profile_length\", data=df)\n",
    "sns_plot.set(title = 'Fitness and Length of Dating Profile', \n",
    "                                                           xlabel = 'Height', \n",
    "                                                           ylabel = 'Number of Words' )\n",
    "sns_plot.figure.savefig('profile_fitness.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT EDITING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20576/20576 [79:41:19<00:00, 10.85s/it]"
     ]
    }
   ],
   "source": [
    "# First, fix conjoined words in the essay\n",
    "# This may take up to 10 minutes\n",
    "df['essay0'] = df['essay0'].progress_apply(split_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['long_words'] = df['essay0'].progress_apply(get_npoly)\n",
    "df['flesch'] = df['essay0'].progress_apply(get_flesch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will the main data file for the rest of the analysis\n",
    "df.to_csv('compressed_okcupid.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
